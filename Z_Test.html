<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="James Van Slyke" />


<title>Z Scores and the z test</title>

<script src="site_libs/header-attrs-2.14/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/darkly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Using R Studio for Statistics</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    R Basics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Why_R.html">Why Use R &amp; R Studio?</a>
    </li>
    <li>
      <a href="Intro_to_R.html">Intro to R</a>
    </li>
    <li>
      <a href="Building_databases.html">Building Databases</a>
    </li>
    <li>
      <a href="More_with_databases.html">More with Databases</a>
    </li>
    <li>
      <a href="Working_with_R_Markdown.html">Working with R Markdown</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Intro to Statistics
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Descriptive_Statistics.html">Descriptive Statistics</a>
    </li>
    <li>
      <a href="Hypothesis_Testing.html">Hypothesis Testing</a>
    </li>
    <li>
      <a href="Probability_Theory.html">Probability Theory</a>
    </li>
    <li>
      <a href="Z_Test.html">z test</a>
    </li>
    <li>
      <a href="Independent_t-test.html">Independent t-test</a>
    </li>
    <li>
      <a href="Paired_samples_t-test.html">Paired Samples t-test</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Graphs
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Intro_ggplot.html">Intro to ggplot</a>
    </li>
    <li>
      <a href="Bar_Graphs_CI.html">Bar Graphs with 95% CIs</a>
    </li>
    <li>
      <a href="Histograms.html">Histograms</a>
    </li>
    <li>
      <a href="Normal_Curve.html">Normal Curve</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Statistical Tests
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Correlation.html">Correlation</a>
    </li>
    <li>
      <a href="Regression.html">Regression</a>
    </li>
    <li>
      <a href="One_Way_ANOVA.html">One-Way ANOVA</a>
    </li>
    <li>
      <a href="Two_Way_ANOVA.html">Two-Way ANOVA</a>
    </li>
    <li>
      <a href="Chi_Square.html">Chi Square</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Z Scores and the z test</h1>
<h4 class="author">James Van Slyke</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="properties-of-the-normal-curve" class="section level3">
<h3>Properties of the Normal Curve</h3>
<div class="figure">
<img
src="https://www.simplypsychology.org/normal-distribution.jpg?ezimgfmt=rs:555x353/rscb26/ng:webp/ngcb26"
alt="" />
<p class="caption">Normal Curve - <a
href="https://www.simplypsychology.org/normal-distribution.html"
class="uri">https://www.simplypsychology.org/normal-distribution.html</a></p>
</div>
<ol style="list-style-type: decimal">
<li>The distribution of scores is symmetrical about the mean, which
indicates that if you were to fold the distribution in half both sides
or tails of the distribution would match.</li>
<li>The mean, median and mode are all equal</li>
<li>The distribution will have two inflection points indicating one
standard deviation above and below the mean.</li>
</ol>
<p>Remember that in a normal curve most of the data is close to the mean
or scores with the highest frequency are closest to the mean. As you
move to the tails of the normal distribution there is a lower frequency
of scores, less of the data set is contained in the tails of the
distribution. Another way to think of this is that more extreme scores
or less probable scores are contained in the tails of the
distribution.</p>
</div>
<div
id="the-proportion-or-percentage-of-data-in-a-normal-curve-is-dispersed-differently-through-the-curve"
class="section level3">
<h3>The proportion or percentage of data in a normal curve is dispersed
differently through the curve</h3>
<div class="figure">
<img src="https://miro.medium.com/max/1400/1*IdGgdrY_n_9_YfkaCh-dag.png"
alt="" />
<p class="caption">Distribution of scores in a normal curve</p>
</div>
<p>Image credit - <a
href="https://towardsdatascience.com/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2"
class="uri">https://towardsdatascience.com/understanding-the-68-95-99-7-rule-for-a-normal-distribution-b7b7cbf760c2</a></p>
<ol style="list-style-type: decimal">
<li>34.13% of the data are between the mean and one standard deviation
above it or below it.</li>
<li>13.59% of the data is between one and two standard deviations</li>
<li>2.15% of the data is between two and three standard deviations</li>
</ol>
<p>So when a data point or score is further on either of the tails of
the distribution, the data point is less likely or more extreme.</p>
<div id="z-scores" class="section level4">
<h4>Z Scores</h4>
<p>One of the ways we are able to understand the probability or
distribution of scores in a normal curve is through the use of <em>z
scores</em>. A z score is score that designates how many standard
deviations a particular score is above or below the mean.</p>
<p>Here is the formula <span class="math display">\[
z = \frac{x - \bar x}{s}
\]</span> z scores can be positive or negative depending on whether the
score in question is above or below the mean. Thus, the higher the
magnitude of the score (meaning independent of whether it’s positive or
negative) the further the score is away from the mean.</p>
<p><img
src="https://cdn.scribbr.com/wp-content/uploads/2020/10/standard-normal-distribution.png"
alt="Z scores" /> Image from <a
href="https://www.scribbr.com/statistics/standard-normal-distribution/"
class="uri">https://www.scribbr.com/statistics/standard-normal-distribution/</a></p>
</div>
</div>
<div id="populations-and-samples" class="section level3">
<h3>Populations and Samples</h3>
<p>So far, we’ve primarily dealt with <em>descriptive statistics</em>,
which are used to describe the basics of a dataset (think central
tendency and variability). However, most of psychological science is
running experiments and testing hypotheses. When psychologists run an
experiment, they use a <em>sample</em>, which is a smaller subgroup of
the population (the larger group) they are trying to explain.</p>
<p>For example, if a psychologist is interested in gender differences,
the populations would be either all females or all males (That’s a large
group of people!). So to run an experiment, psychologists instead use a
sample (smaller subgroup) of males and a sample of females to compare
them based on some particular variable.</p>
<p>The process of using a sample to infer characteristics about a
population is called <em>inferential statistics</em>. Most of the
statistics used to analyze experiments are inferential statistics of one
kind or another.</p>
<p>The first test we’ll learn in inferential statistics is the <em>z
test</em>. It’s not used very often in statistics generally, but it lays
out a lot of the conceptual issues involved in statistical tests.</p>
<div id="example-problem" class="section level4">
<h4>Example Problem</h4>
<p>Suppose you work for a gasoline company and you’ve developed a new
additive that you believe increase gas mileage. Your supervisor would
like to add it to their gasoline, but feels that it’s important to run a
test first to see if the additive really increases gas mileage. So you
collect a sample of 75 cars that are all using the gasoline additive.
You discover that the mean gas mileage for your sample is 26.5 miles per
gallon. <span class="math display">\[ \bar x = 26.5 \]</span></p>
<p>Looking back as some old records you find that over the years your
company’s gasoline mileage has averaged about <em>24.7 miles per
gallon</em>. On the face of it, it looks like your additive has
increased the gasoline mileage, <em>but is the difference statistically
significant?</em></p>
</div>
</div>
<div id="hypotheses" class="section level3">
<h3>Hypotheses</h3>
<p>A good experiment begins with a good hypothesis, which clearly
specifies the variables in an experiment. There are two types of
variables in most experiments:</p>
<p>1. Independent Variable (IV)</p>
<p>2. Dependent Variable (DV)</p>
<blockquote>
<p>The independent variable is the variable that is manipulated in some
way and is thought to be the cause in the experiment, while the
dependent variable is measured to assess whether the independent
variable produces any changes in it, so it is thought to be the effect
in an experiment.</p>
</blockquote>
<p>The hypothesis specifies the relationship between the IV and the DV.
In statistics, there are two types of hypotheses, the null hypothesis
and the alternative hypothesis. The null hypothesis negates or
contradicts the expected relationship between the IV and DV if the IV
has a real effect. The alternative hypothesis specifies the relationship
between the IV and DV as the experimenter expects it to be, the IV
causing a change in the DV.</p>
<p>In experimental methodology, the <em>null</em> hypothesis is directly
tested and if it can be shown to be <em>unsupported</em> (based on the
statistical analysis), it then supports its counterpart, the alternative
hypothesis.</p>
<p>Here are the two hypotheses for the gasoline additive example</p>
<blockquote>
<p>Null hypothesis - “The gasoline additive will <em>not</em> increase
the mileage of the gasoline”</p>
</blockquote>
<blockquote>
<p>Alternative Hypothesis - “The gasoline additive will increase the
mileage of the gasoline”</p>
</blockquote>
<p>Notice that the hypotheses are basically exactly the same, except
that the null hypothesis <em>negates</em> the relationship between the
IV and DV. A good hypothesis will include both the IV and DV and
something to specify the expected outcome of the experiment. In this
case the word “increases” shows the relationship between the two.</p>
<p>This is an example of a <em>directional</em> hypothesis because it’s
specifying the direction of the expected effect of the IV. A
nondirectional hypothesis does not specify the direction it just states
that the IV will have an <em>effect</em> on the DV. Here’s how gasoline
mileage hypotheses would look like as <em>non</em>directional
hypothesis.</p>
<blockquote>
<p>Null hypothesis - “The gasoline additive will <em>not</em> effect the
mileage of the gasoline”</p>
</blockquote>
<blockquote>
<p>Alternative Hypothesis - “The gasoline additive will effect the
mileage of the gasoline”</p>
</blockquote>
<p>Notice that the only real difference is that the word “increase” was
swapped with the word “effect”. Whether the hypothesis is directional or
nondirectional is usually determined by the experimenter or the context
of the experiment.</p>
<div class="figure">
<img
src="https://miro.medium.com/max/2000/1*SsmEKrsKn03e3cxdoqOcNA.jpeg"
alt="" />
<p class="caption">Nondirectional vs. Directional tests</p>
</div>
<p>Image from <a
href="https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab"
class="uri">https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab</a></p>
<p>Directional tests are one-tailed (evaluate only one tail of the
distribution) because they are investigating a specific direction of
effect for the IV (e.g. increasing) while nondirectional tests are
two-tailed (evaluate both tails of the distribution) because they are
not specifying the direction of effect for IV.</p>
<p>Basic scientific methodology requires an experimental and a control
group. The experimental group receives the IV while the control group
does not. If a difference between the groups can be detected then that
difference may be attributable to the IV. If there is no difference
between the groups the IV does not have an effect.</p>
<p>Another way to state this is that the null hypothesis assumes that
there is no difference between the control and experimental groups,
while the alternative hypothesis assumes that there will be a difference
between the groups.</p>
<blockquote>
<p>Null hypothesis - experimental group = control group</p>
</blockquote>
<blockquote>
<p>Alternative Hypothesis - experimental group <span
class="math inline">\(\neq\)</span> control group</p>
</blockquote>
<p>For the z test, the collected sample is the experimental group, while
the population stands in for the control group. For the gasoline
additive example, the sample of cars tested with the additive added to
the gasoline is the experimental group because the IV (gasoline
additive) was added to that group, while the population is the control
group because it consists of older records of the average gas mileage
without the additive.</p>
<div id="the-sampling-distribution-of-the-mean" class="section level4">
<h4>The Sampling Distribution of the Mean</h4>
<p>For the z test, the sample mean <span class="math inline">\(\bar
x\)</span> is compared to the population mean <span
class="math inline">\(\mu\)</span> to see if there is a difference
between the two values that could be attributable to the IV. The null
hypothesis assumes that the population mean and sample mean are equal,
while the alternative hypothesis assumes that they are different.</p>
<blockquote>
<p>Null hypothesis - <span class="math inline">\(\mu = \bar
x\)</span></p>
</blockquote>
<blockquote>
<p>Alternative hypothesis - <span class="math inline">\(\mu \neq \bar
x\)</span></p>
</blockquote>
<p>In our example we already know that the two means are different, the
sample mean is 26.5, while the population mean is 24.7. The problem is
that we don’t know if the difference is statistically significant or
just the result of <em>measurement error</em>.</p>
<p>For example, imagine that you weigh yourself every day for a whole
week. Most likely there will be fluctuations in your weight during the
week. It will be slightly higher one day and slightly lower another day
with an average or mean that stays more consistent. So an analysis is
needed to make sure that the differences we observe in our two groups
(experimental and control groups or in this case population mean and
sample mean) is not just the result of random differences between the
mean values (measurement error). The null hypothesis assumes that our
findings are simply the result of random chance, while the alternative
assumes that the differences are attributable to the work of the
independent variable.</p>
<p>Probability is used to ascertain the likelihood that our results are
due to the work of the IV. If it can be shown that the sample we’ve
collected (in this case the sample with the additive added to the
gasoline) could reasonably be assumed to have come from the comparison
population (old records of gasoline without the additive), then the Null
hypothesis must be true and the means are essentially the same <span
class="math inline">\(\mu = \bar x\)</span>.</p>
<p>If, however, we can demonstrate that it is unreasonable to assume
that our collected sample with the independent variable (gasoline
additive) did not come from the comparison population (gasoline without
the additive) then we can instead reject the null and lend support to
the alternative hypothesis.</p>
<p>In order to accomplish this we use the <em>sampling distribution of
the mean (SDM)</em>, which essentially provides a distribution of
potential sample means drawn from a population. Imagine that you had a
large population and you began selecting samples from it and each sample
had an n = 75. For each sample you calculated the mean and that
collection of sample means (let’s say you collected 1000 samples) became
it’s own distribution or collection of scores. This new distribution of
scores is the SDM. It’s a distribution of sample means drawn from a
particular population.</p>
<p>Based on the mysterious intricacies of statistics, we know certain
properties of the SDM based on the <em>central limit theorem</em>. Those
are:</p>
<p>1. <span class="math inline">\(\mu = \mu_{\bar x}\)</span></p>
<p>2. <span class="math inline">\(\sigma_{\bar x} =
\frac{\sigma}{\sqrt{n}}\)</span></p>
<p>3. The SDM will be <em>more narrow</em> than the population</p>
<p>First, the mean of the SDM <span class="math inline">\(\mu_{\bar
x}\)</span> will be equal to the mean of the population <span
class="math inline">\(\mu\)</span>. Second, the standard deviation of
the SDM <span class="math inline">\(\sigma_{\bar x}\)</span> will be
equal to the population standard deviation <span
class="math inline">\(\sigma\)</span> divided by the square root of n
(the number of scores collected in each sample). This leads to the third
property. Since the SDM is constructed of sample means a certain amount
of variation has been removed causing the shape of the curve to be more
narrow than the population.</p>
<p>The SDM functions as a probability curve or distribution of potential
sample means drawn from a population. Based on what we know about normal
curves, most of the potential sample means would be between the mean and
one standard deviation (34.13% on either side of the mean) above or
below the mean. Then a smaller percentage between 1 and 2 standard
deviations and even smaller between 2 and 3. Thus the samples with the
lowest probability of coming from the population would be on the tails
of the distribution.</p>
<p>The scientific standard for determining the probability of a sample
coming from a population is determined by alpha. <span
class="math display">\[
\alpha = .05
\]</span> Thus the standard for determining the probability of a sample
coming from a population is 5% or in order for statistical results to be
considered significant it must be demonstrated that there is only a 5%
chance or less that the sample came from the original population.</p>
<p>In order to analyze the probability associated with a particular
sample mean, z scores are used to locate the sample mean on the SDM and
calculate the probability that it came from the comparison
population.</p>
</div>
</div>
<div id="computing-the-z-test-using-r" class="section level3">
<h3>Computing the z test using R</h3>
<p><strong>Gasoline mileage example</strong></p>
<p>First find the value of z</p>
<p><em>z formula</em></p>
<p><span class="math display">\[
z \;test = \frac{x - \mu}{\frac{\sigma}{\sqrt{n}}}
\]</span></p>
<p>The formula written out is: (mean of the sample - mean of the
population)/standard error of the mean). The standard error of the mean
is the standard deviation of the population divided by the square root
of the number in the sample.</p>
<p>In R you can run the formula like this:</p>
<pre class="r"><code>z_score_gasoline &lt;- (26.5-24.7)/(4.8/sqrt(75))
z_score_gasoline</code></pre>
<pre><code>## [1] 3.247595</code></pre>
<p><em>Define region for rejection of the Null Hypothesis</em></p>
<p>This values is known as alpha, alpha = .05 or 5%</p>
<p><em>What is the probability associated with the z score?</em></p>
<p>We use “pnorm” command to find probability associated with a
particular z score. The “abs” stands for the absolute value because
whether the z score is positive or negative is irrelevant for the
analysis.</p>
<p>Here’s how it is coded in R Studio:</p>
<pre class="r"><code>pnorm(abs(z_score_gasoline))</code></pre>
<pre><code>## [1] 0.9994181</code></pre>
<p>This gives us the probability associated with that z score, which in
this case is 99.94%. Remember that in this instance, we are analyzing
the probably that this sample mean came from the original population. To
accomplish this we analyze the tail of the distribution, so we want the
probability from the z score and higher, which will be our <em>p</em>
value, so we subtract 1 from the probability associated with the z
score.</p>
<pre class="r"><code>1-pnorm(abs(z_score_gasoline))</code></pre>
<pre><code>## [1] 0.0005819235</code></pre>
<p>Finally we compare the <em>p</em> value to alpha. If the p value is
less than alpha (.05), then we can reject the Null hypothesis.</p>
<blockquote>
<p>.0006 &lt; .05</p>
</blockquote>
<p>Since our p value is less than .05 we reject the null and can accept
the alternative hypothesis.</p>
<p><em>What if we have a nondirectional test?</em></p>
<p>To account for both tails of the distribution, simply multiply by
2</p>
<pre class="r"><code>(1-pnorm(abs(z_score_gasoline)))*2</code></pre>
<pre><code>## [1] 0.001163847</code></pre>
<p>A second way to investigate whether a sample came from a particular
population is to use a confidence interval (CI). Confidence intervals
describe the range of possible population means from which a sample
could be drawn. The confidence interval used most often is the 95%
confidence interval, which, in this case, would state that 95% of
population means would be contained in the identified range. Here is the
formula for the 95% CI for a z test.</p>
<p>lower boundary of CI - <span class="math inline">\(\bar x -
1.96\)</span> x <span class="math inline">\(\sigma_{\bar x}\)</span></p>
<p>upper boundary of CI - <span class="math inline">\(\bar x +
1.96\)</span> x <span class="math inline">\(\sigma_{\bar x}\)</span></p>
<p>So here is how the calculation is performed in R Studio</p>
<pre class="r"><code>Lower &lt;- 26.5 - 1.96*(4.8/sqrt(75))
Lower</code></pre>
<pre><code>## [1] 25.41366</code></pre>
<pre class="r"><code>Upper &lt;- 26.5 + 1.96*(4.8/sqrt(75))
Upper</code></pre>
<pre><code>## [1] 27.58634</code></pre>
<p>We write confidence intervals like this: 26.5, 95% CI [25.41,
27.59].</p>
<p>Notice that the original population mean used for comparison to the
sample was 24.7, which is outside the range provided for the 95% CI,
further confirming our conclusion to reject the null.</p>
<p>In addition to the findings from the z test, it’s always important to
include an effect size as well. Effect sizes help us to analyze whether
the results are meaningful or not and allows us to compare across
different types of statistical tests.</p>
<p>For mean differences use <em>Cohen’s d</em> <span
class="math display">\[
Cohen&#39;s\;d = \frac{\bar x - \mu}{\sigma}
\]</span> So the formula is: (mean of the sample - mean of the
population)/standard deviation of the population</p>
<p>In R Studio</p>
<pre class="r"><code>Cohens_d &lt;- (26.5-24.7)/4.8
Cohens_d</code></pre>
<pre><code>## [1] 0.375</code></pre>
<p>The full conclusion should be stated as:</p>
<blockquote>
<p>The sample mean <em>M</em> = 26.5, <em>95% CI</em> [25.41, 27.59],
was significantly different [mean difference = 1.8] from the population
mean <em>M</em> = 24.7, based on a <em>z</em> = 3.25; <em>p</em> =
0.0006, <em>d</em> = .38. Reject the null, the gasoline additive does
increase gas mileage.</p>
</blockquote>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
