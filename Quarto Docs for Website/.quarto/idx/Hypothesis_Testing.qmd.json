{"title":"Hypothesis Testing","markdown":{"yaml":{"title":"Hypothesis Testing","author":"James Van Slyke"},"headingText":"Populations and Samples","containsRefs":false,"markdown":"\n\n\nSo far, we've primarily dealt with *descriptive statistics*, which are used to describe the basics of a dataset (think central tendency and variability). However, most of psychological science is running experiments and testing hypotheses. When psychologists run an experiment, they use a *sample*, which is a smaller subgroup of the population (the larger group) they are trying to explain. Most experiments are run on samples, which serve as an estimation of some property of the population that a scientist is attempting to explain.\n\nFor example, if a psychologist is interested in gender differences, the populations would be either all females or all males (That's a large group of people!). So to run an experiment, psychologists instead use a sample (smaller subgroup) of males and a sample of females to compare them based on some particular variable.\n\nThe process of using a sample to infer characteristics about a population is called *inferential statistics*. Many of the statistics used to analyze experiments are inferential statistics of one kind or another.\n\n### Hypotheses\n\nA good experiment begins with a good hypothesis, which clearly specifies the variables in an experiment. There are two types of variables in most experiments:\n\n1.  Independent Variable (IV)\n\n2.  Dependent Variable (DV)\n\n> The independent variable is the variable that is manipulated in some way and is thought to be the cause in the experiment, while the dependent variable is measured to assess whether the independent variable produces any changes in it, so it is thought to be the effect in an experiment.\n\nThe hypothesis specifies the relationship between the IV and the DV. In statistics, there are two types of hypotheses, the *null hypothesis* and the *alternative hypothesis*. The null hypothesis negates or contradicts the expected relationship between the IV and DV if the IV has a real effect. The alternative hypothesis specifies the relationship between the IV and DV as the experimenter expects it to be, the IV causing a change in the DV.\n\n### Null and Alternative Hypotheses\n\nIn experimental methodology, the *null* hypothesis is directly tested and if it can be shown to be *unsupported* (based on the statistical analysis), it then lends support to its counterpart, the alternative hypothesis.\n\nFor example, suppose you work for a gasoline company and you've developed a new additive that you believe increases gas mileage. Your supervisor would like to add it to their gasoline, but feels that it's important to run a test first to see if the additive really increases gas mileage. So you collect a sample of 75 cars that are all using the gasoline additive. You discover that the mean gas mileage for your sample is 26.5 miles per gallon.\n\nHere are the two hypotheses for the gasoline additive example.\n\n> Null hypothesis - \"The gasoline additive will *not* increase the mileage of the gasoline\"\n\n> Alternative Hypothesis - \"The gasoline additive will increase the mileage of the gasoline\"\n\nNotice that the hypotheses are basically exactly the same, except that the null hypothesis *negates* the relationship between the IV and DV. A good hypothesis will include both the IV and DV and something to specify the expected outcome of the experiment. In this case the word \"increases\" shows the relationship between the two variables.\n\n### Directional or Nondirectional?\n\nThis is an example of a *directional* hypothesis because it's specifying the direction of the expected effect of the IV. A nondirectional hypothesis does not specify the direction, it just states that the IV will have an *effect* on the DV. Here's how gasoline mileage hypotheses would look like as *non*directional hypothesis.\n\n> Null hypothesis - \"The gasoline additive will *not* effect the mileage of the gasoline\"\n\n> Alternative Hypothesis - \"The gasoline additive will effect the mileage of the gasoline\"\n\nNotice that the only real difference is that the word \"increase\" was swapped with the word \"effect\". Whether the hypothesis is directional or nondirectional is usually determined by the experimenter or the context of the experiment.\n\n![Nondirectional vs. Directional tests](https://miro.medium.com/max/2000/1*SsmEKrsKn03e3cxdoqOcNA.jpeg)\n\nImage from <https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab>\n\nDirectional tests are one-tailed (evaluate only one tail of the distribution) because they are investigating a specific direction of effect for the IV (e.g. increasing) while nondirectional tests are two-tailed (evaluate both tails of the distribution) because they are not specifying the direction of effect for IV.\n\n### Experimental and Control Groups\n\nBasic scientific methodology requires an experimental and a control group. The experimental group receives the IV while the control group does not. If a difference between the groups can be detected then that difference may be attributable to the IV. If there is no difference between the groups the IV does not have an effect.\n\nAnother way to state this is that the null hypothesis assumes that there is no difference between the control and experimental groups (the groups or equal), while the alternative hypothesis assumes that there will be a difference between the groups.\n\n> Null hypothesis - experimental group = control group\n\n> Alternative Hypothesis - experimental group $\\neq$ control group\n\nThe null hypothesis is tested to answer the question, \"Are these results due to chance?\" Different statistical tests use different probability distributions to analyze the results of an experiment. The probability distributions allow us to see what is the probability associated with the results we obtained in our experiment. \n\n### Alpha\nThe probability that is shown through the probability distribution is compared to *alpha*. Alpha is an accepted scientific standard for determining the likelihood of rejecting the null hypothesis. Alpha is equal to .05 or 5%. So the standard that is accepted by the scientific community is that there is only a 5% chance or less that the findings in the experiment conducted are the result of chance alone. \n$$\n\\alpha = .05\n$$ \nThe probability associated with the outcome of our experiment based on a probability distribution is known as the *p* value (*p* standing for probability)\n\n### Basic Statistical Formula\n\nA really easy way to understand how most statistical outputs are generated is to understand this simple formula. \n$$\n\\frac{signal}{noise}\n$$ \nSignal refers to what is known as systematic variation or variation that is introduced by an experiment. In the gasoline example, the additive that is supposed to increase gas mileage is systematic variation. The gasoline is being manipulated to try to increase gas mileage through the use of the additive. The more increase in gas mileage associated with the additive, the stronger the signal and the more likely the independent variable (in this case the additive) has a real effect.\n\nNoise is like background noise or variation that exists, but wasn't introduced through the experimental manipulation. This is often referred to as unsystematic variation. This is variation that exists for a variety of reasons. Going back to our gasoline mileage example, unsystematic variation could be differences in the cars used, differences in the road conditions or differences in the weather. Experimentors try to minimize this type of variation as much as possible, but there is always a certain amount of unsystematic variation\n\n### Measurement Error\nOne potential form of unsystematic variation is *measurement error*. For example, imagine that you weigh yourself every day for a whole week. Most likely there will be fluctuations in your weight during the week. It will be slightly higher one day and slightly lower another day with an average or mean that stays more consistent. Measurement error exists in any variable that we are attempting to measure. It is unavoidable to a certain extent because whatever we are attempting to measure, there will be certain fluctuations in the precision of measurement. Going back to our original formula, measurement error is our estimation of noise and although experiments are always trying to minimize error a certain amount will always be present. So another way to understand our primary formula is \n$$\nstatistics = \\frac{experimental\\;manipulation}{measurement\\;error}\n$$\n\n### Confidence Intervals\n\nOne statistic to help analyze measurement error is called a *confidence interval*. A confidence interval is an interval of numbers between which we are 95% confident that the population mean is contained when drawn from a sample. According to Morling (2021) a confidence interval is composed of 3 components.\n\n1.  Variability component - this is most often the standard deviation *sd* (remember that the *sd* if one of our basic measures of dispersion or variability)\n2.  Sample size component - Usually this will be a calculation involving the number in the sample, most often symbolized by *n*.\n3.  Constant associated with 95% - Here we'll use the z score associated with 95 percent\n\nHere is the basic formula for a confidence interval using z scores.\n\nlower boundary of CI - $\\bar X - 1.96$ x $\\frac{s}{\\sqrt{n}}$\n\nupper boundary of CI - $\\bar X + 1.96$ x $\\frac{s}{\\sqrt{n}}$\n\n$\\frac{s}{\\sqrt{n}}$ is known as the standard error of the mean and is the standard deviation divided by the square root of the number in the sample. 1.96 is the z score associated with the 95% percentile, thus, the 95% confidence interval.\n\n#### Confidence Interval Example\n\nLet's imagine we have a sample of persons aged 40 to 50 who were trying to run on a treadmill at it's fastest speed. This is a sample of times in seconds they were able to maintain a sprint at the treadmill's fastest speed.\n\n```{r}\nsprint <- c(18, 16, 18, 24, 23, 22, 22, 23, 26, 29, 32, 34, 34, 36, 36, 43, 42, 49, 46, 46, 57)\n```\n\nThen we can find the mean and standard deviation for this group of scores\n\n```{r}\nsprint_mean <- mean(sprint)\nsprint_sd <- sd(sprint)\n```\n\nThen we can use these scores to find the 95% confidence interval\n\n```{r}\nlower <- sprint_mean - 1.96*sprint_sd/sqrt(21)\nupper <- sprint_mean + 1.96*sprint_sd/sqrt(21)\nlower\nupper\n```\n\nBased on this sample, we are 95% confident that the mean time for the population of 40 to 50-year-olds to sprint on the treadmill at its top speed is between 27.23 and 37.15 seconds.\n","srcMarkdownNoYaml":"\n\n### Populations and Samples\n\nSo far, we've primarily dealt with *descriptive statistics*, which are used to describe the basics of a dataset (think central tendency and variability). However, most of psychological science is running experiments and testing hypotheses. When psychologists run an experiment, they use a *sample*, which is a smaller subgroup of the population (the larger group) they are trying to explain. Most experiments are run on samples, which serve as an estimation of some property of the population that a scientist is attempting to explain.\n\nFor example, if a psychologist is interested in gender differences, the populations would be either all females or all males (That's a large group of people!). So to run an experiment, psychologists instead use a sample (smaller subgroup) of males and a sample of females to compare them based on some particular variable.\n\nThe process of using a sample to infer characteristics about a population is called *inferential statistics*. Many of the statistics used to analyze experiments are inferential statistics of one kind or another.\n\n### Hypotheses\n\nA good experiment begins with a good hypothesis, which clearly specifies the variables in an experiment. There are two types of variables in most experiments:\n\n1.  Independent Variable (IV)\n\n2.  Dependent Variable (DV)\n\n> The independent variable is the variable that is manipulated in some way and is thought to be the cause in the experiment, while the dependent variable is measured to assess whether the independent variable produces any changes in it, so it is thought to be the effect in an experiment.\n\nThe hypothesis specifies the relationship between the IV and the DV. In statistics, there are two types of hypotheses, the *null hypothesis* and the *alternative hypothesis*. The null hypothesis negates or contradicts the expected relationship between the IV and DV if the IV has a real effect. The alternative hypothesis specifies the relationship between the IV and DV as the experimenter expects it to be, the IV causing a change in the DV.\n\n### Null and Alternative Hypotheses\n\nIn experimental methodology, the *null* hypothesis is directly tested and if it can be shown to be *unsupported* (based on the statistical analysis), it then lends support to its counterpart, the alternative hypothesis.\n\nFor example, suppose you work for a gasoline company and you've developed a new additive that you believe increases gas mileage. Your supervisor would like to add it to their gasoline, but feels that it's important to run a test first to see if the additive really increases gas mileage. So you collect a sample of 75 cars that are all using the gasoline additive. You discover that the mean gas mileage for your sample is 26.5 miles per gallon.\n\nHere are the two hypotheses for the gasoline additive example.\n\n> Null hypothesis - \"The gasoline additive will *not* increase the mileage of the gasoline\"\n\n> Alternative Hypothesis - \"The gasoline additive will increase the mileage of the gasoline\"\n\nNotice that the hypotheses are basically exactly the same, except that the null hypothesis *negates* the relationship between the IV and DV. A good hypothesis will include both the IV and DV and something to specify the expected outcome of the experiment. In this case the word \"increases\" shows the relationship between the two variables.\n\n### Directional or Nondirectional?\n\nThis is an example of a *directional* hypothesis because it's specifying the direction of the expected effect of the IV. A nondirectional hypothesis does not specify the direction, it just states that the IV will have an *effect* on the DV. Here's how gasoline mileage hypotheses would look like as *non*directional hypothesis.\n\n> Null hypothesis - \"The gasoline additive will *not* effect the mileage of the gasoline\"\n\n> Alternative Hypothesis - \"The gasoline additive will effect the mileage of the gasoline\"\n\nNotice that the only real difference is that the word \"increase\" was swapped with the word \"effect\". Whether the hypothesis is directional or nondirectional is usually determined by the experimenter or the context of the experiment.\n\n![Nondirectional vs. Directional tests](https://miro.medium.com/max/2000/1*SsmEKrsKn03e3cxdoqOcNA.jpeg)\n\nImage from <https://towardsdatascience.com/hypothesis-testing-z-scores-337fb06e26ab>\n\nDirectional tests are one-tailed (evaluate only one tail of the distribution) because they are investigating a specific direction of effect for the IV (e.g. increasing) while nondirectional tests are two-tailed (evaluate both tails of the distribution) because they are not specifying the direction of effect for IV.\n\n### Experimental and Control Groups\n\nBasic scientific methodology requires an experimental and a control group. The experimental group receives the IV while the control group does not. If a difference between the groups can be detected then that difference may be attributable to the IV. If there is no difference between the groups the IV does not have an effect.\n\nAnother way to state this is that the null hypothesis assumes that there is no difference between the control and experimental groups (the groups or equal), while the alternative hypothesis assumes that there will be a difference between the groups.\n\n> Null hypothesis - experimental group = control group\n\n> Alternative Hypothesis - experimental group $\\neq$ control group\n\nThe null hypothesis is tested to answer the question, \"Are these results due to chance?\" Different statistical tests use different probability distributions to analyze the results of an experiment. The probability distributions allow us to see what is the probability associated with the results we obtained in our experiment. \n\n### Alpha\nThe probability that is shown through the probability distribution is compared to *alpha*. Alpha is an accepted scientific standard for determining the likelihood of rejecting the null hypothesis. Alpha is equal to .05 or 5%. So the standard that is accepted by the scientific community is that there is only a 5% chance or less that the findings in the experiment conducted are the result of chance alone. \n$$\n\\alpha = .05\n$$ \nThe probability associated with the outcome of our experiment based on a probability distribution is known as the *p* value (*p* standing for probability)\n\n### Basic Statistical Formula\n\nA really easy way to understand how most statistical outputs are generated is to understand this simple formula. \n$$\n\\frac{signal}{noise}\n$$ \nSignal refers to what is known as systematic variation or variation that is introduced by an experiment. In the gasoline example, the additive that is supposed to increase gas mileage is systematic variation. The gasoline is being manipulated to try to increase gas mileage through the use of the additive. The more increase in gas mileage associated with the additive, the stronger the signal and the more likely the independent variable (in this case the additive) has a real effect.\n\nNoise is like background noise or variation that exists, but wasn't introduced through the experimental manipulation. This is often referred to as unsystematic variation. This is variation that exists for a variety of reasons. Going back to our gasoline mileage example, unsystematic variation could be differences in the cars used, differences in the road conditions or differences in the weather. Experimentors try to minimize this type of variation as much as possible, but there is always a certain amount of unsystematic variation\n\n### Measurement Error\nOne potential form of unsystematic variation is *measurement error*. For example, imagine that you weigh yourself every day for a whole week. Most likely there will be fluctuations in your weight during the week. It will be slightly higher one day and slightly lower another day with an average or mean that stays more consistent. Measurement error exists in any variable that we are attempting to measure. It is unavoidable to a certain extent because whatever we are attempting to measure, there will be certain fluctuations in the precision of measurement. Going back to our original formula, measurement error is our estimation of noise and although experiments are always trying to minimize error a certain amount will always be present. So another way to understand our primary formula is \n$$\nstatistics = \\frac{experimental\\;manipulation}{measurement\\;error}\n$$\n\n### Confidence Intervals\n\nOne statistic to help analyze measurement error is called a *confidence interval*. A confidence interval is an interval of numbers between which we are 95% confident that the population mean is contained when drawn from a sample. According to Morling (2021) a confidence interval is composed of 3 components.\n\n1.  Variability component - this is most often the standard deviation *sd* (remember that the *sd* if one of our basic measures of dispersion or variability)\n2.  Sample size component - Usually this will be a calculation involving the number in the sample, most often symbolized by *n*.\n3.  Constant associated with 95% - Here we'll use the z score associated with 95 percent\n\nHere is the basic formula for a confidence interval using z scores.\n\nlower boundary of CI - $\\bar X - 1.96$ x $\\frac{s}{\\sqrt{n}}$\n\nupper boundary of CI - $\\bar X + 1.96$ x $\\frac{s}{\\sqrt{n}}$\n\n$\\frac{s}{\\sqrt{n}}$ is known as the standard error of the mean and is the standard deviation divided by the square root of the number in the sample. 1.96 is the z score associated with the 95% percentile, thus, the 95% confidence interval.\n\n#### Confidence Interval Example\n\nLet's imagine we have a sample of persons aged 40 to 50 who were trying to run on a treadmill at it's fastest speed. This is a sample of times in seconds they were able to maintain a sprint at the treadmill's fastest speed.\n\n```{r}\nsprint <- c(18, 16, 18, 24, 23, 22, 22, 23, 26, 29, 32, 34, 34, 36, 36, 43, 42, 49, 46, 46, 57)\n```\n\nThen we can find the mean and standard deviation for this group of scores\n\n```{r}\nsprint_mean <- mean(sprint)\nsprint_sd <- sd(sprint)\n```\n\nThen we can use these scores to find the 95% confidence interval\n\n```{r}\nlower <- sprint_mean - 1.96*sprint_sd/sqrt(21)\nupper <- sprint_mean + 1.96*sprint_sd/sqrt(21)\nlower\nupper\n```\n\nBased on this sample, we are 95% confident that the mean time for the population of 40 to 50-year-olds to sprint on the treadmill at its top speed is between 27.23 and 37.15 seconds.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Hypothesis_Testing.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","editor":"visual","theme":{"light":"minty","dark":"darkly"},"title":"Hypothesis Testing","author":"James Van Slyke"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}