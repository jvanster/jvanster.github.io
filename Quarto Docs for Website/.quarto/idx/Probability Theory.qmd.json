{"title":"Probability Theory","markdown":{"yaml":{"title":"Probability Theory","author":"James Van Slyke"},"headingText":"Introduction to Probability","containsRefs":false,"markdown":"\n\n\nThe foundation of statistics is probability, which is analyzing the chances that an event will or will not occur. Here is the basic formula for probability:\n\n$$ p(A) = \\frac{Number\\ of\\ events\\ classifiable\\ as\\ A}{Total\\ number\\ of\\ possible\\ events} $$\n\nWe can start with a basic example using a deck of 52 playing cards. What's the probability of drawing an Ace?\n\n1.  What are the number of events classifiable as Ace?\n\nA deck of playing cards has 4 aces of the four suits, hearts, diamonds, clubs, and spades. So our numerator is 4.\n\nThe total number of possible events or total number in a deck of cards is 52. So our denominator is 52. So we can write the equation like this: $$ p (Ace)=\\frac{4}{52}$$\n\nWe can use R to find this probability\n\n```{r}\n4/52\n```\n\nProbabilities are always given in proportions or numbers between 0 and 1. If a given probability is 0 it is not possible for a particular event to occur while if a given probability is 1 it is certain that a particular event will occur. For example the probability of getting a 7 on one roll of a 6 sided die is 0 because the only possible outcomes on one roll of a six sided die is between 1 and 6. Whereas when you add together the probabilities of rolling a 1 through 6 together you get 1, because that is the total number of possible outcomes for a six sided die. To demonstrate, the probability of rolling a \"4\" on a six-sided die is one out of six or: $$p(4) = \\frac{1}{6}$$ Or in R\n\n```{r}\n1/6\n```\n\nSo if we add together all the probabilities for each side of the die we'll get 1. $$p(1) = \\frac{1}{6} + p(2) = \\frac{1}{6} + p(3) = \\frac{1}{6} + p(4) = \\frac{1}{6} + p(5) = \\frac{1}{6} + p(6) = \\frac{1}{6} = 1$$ Or in R\n\n```{r}\n1/6+1/6+1/6+1/6+1/6+1/6\n```\n\n### The Rules of Probability\n\nThere are two basic rules to probability, the *addition* rule and the *multiplication* rule. The addition rule applies to the single occurrence of two or more events. For example, what is the probability of drawing an Ace or a King on single draw from a deck of cards. In this case we *add* the probability of drawing an Ace and a King together to find the correct probability. Recall, that the probability for drawing an Ace looks like this:\n\n$$ p (Ace)=\\frac{4}{52}$$\n\nThe probability of drawing a king would be the same as an Ace because there are 4 Kings of each suit in the deck of cards, so the formula looks like this:\n\n$$ p (Ace\\ or\\ a\\ King)=\\frac{4}{52} + \\frac{4}{52}$$\n\nSo the probability of drawing an Ace or a King would be $\\frac{8}{52}$.\n\nUsing R we would find\n\n```{r}\n4/52+4/52\n```\n\nThe *multiplication rule* applies for more than one draw or successive events. For example, what would be the probability of drawing an Ace on the first draw and a King on the second. In that case, you would mutlitply rather than add.\n\n$$ p (Ace\\ and\\ a\\ King)=\\frac{4}{52} \\times \\frac{4}{51}$$\n\nNotice, how in the second fraction the denominator is 51 rather than 52 to account for the fact that after you've drawn the Ace there are only 51 cards left. In this case, the second card is drawn without replacement.\n\n```{r}\n4/52*4/51\n```\n\n### Binomial Distribution\n\nMathematics creates the ability to go beyond probabilities for single events and look at probability distributions or a set of probability values based on a certain number of events. Statistics is based on probability distributions that assign a particular probability to an observed outcome. In psychological science, probability distributions are used to analyze the probability of a particular outcome observed in an experiment. Different probability distributions are used for various statistical tests such as the *t test*, *analysis of variance* or *ANOVA*, and the *chi-square test*.\n\nA good place to begin to understand probability distributions is the binomial distribution. For example, imagine you are flipping an evenly weighted coin. What's the probability of getting heads? Let's go back to our original formula.\n\n-   Number of events classifiable as \"heads\" = 1\n\n-   Total number of possible events = 2\n\nHere's the formula:\n\n$$p(heads) = \\frac{1}{2}$$\n\nWe can use R to get the proportion.\n\n```{r}\n1/2\n```\n\nWhat about the probability of getting 2 heads in a row on 2 flips of the coin? To analyze the probability of successive events or outcomes you need to multiply the probability for each event. This is called the *multiplication rule* for probability. So the formula looks like:\n\n$$p(2heads) = \\frac{1}{2}\\times\\frac{1}{2} = \\frac{1}{4}$$\n\nUsing r we find:\n\n```{r}\n1/2*1/2\n```\n\nSo we could go on and figure out the probability for 3 heads and 4 heads and so on, but instead the binomial distribution can provide a distribution of different probability values based on a given number of events. The binomial distribution is a just an extension of the mathematics we would need to do by hand to find the probability for different events and luckily, as usual, R is able to do this for us.\n\nThe function we'll use is called `dbinom()`. The main arguments for the function are:\n\n`x` The number of outcomes for the given probability currently being calculated\n\n`size` The number of the overall size of the experiment\n\n`prob` The given probability value\n\nSo let's calculate the original probability problem of two heads in two flips of a fair-sided coin. Notice that the answer is the same to the formula we used earlier.\n\n```{r}\ndbinom(x=2, size = 2, prob = 1/2)\n```\n\nProbabilities can be listed as fractions or proportions\n\n```{r}\ndbinom(x=2, size = 2, prob = 0.5)\n```\n\nWe can also use it for other probability values such as rolling a six sided die, which has the probability value of $\\frac{1}{6}$\n\n```{r}\ndbinom(x=1, size = 1, prob = 1/6)\n```\n\nOf course, it's most helpful for probabilities for larger numbers of events. Like what's the probability of rolling 4 sixes over 20 trials.\n\n```{r}\ndbinom(x =4, size = 20, prob = 1/6)\n```\n\nWe can also represent this as distribution graph.\n\n```{r}\nsuccess <- 0:20\nplot(success, dbinom(success, size=20, prob=1/6),type='h',\n     col = \"blue\")\n```\n\nEach of the lines represents the probability for a given outcome. Notice how the outcome for rolling 4 sixes in 20 trials is around 0.20, which was found in the original calculation using the `dbinom()` function. The highest probability is to roll a \"6\" three times out of 20 rolls of the dice. Notice how the graph of the binomial distribution shows us the same answer as using the `dbinom()` function.\n\n```{r}\ndbinom(x=3, size = 20, prob = 1/6)\n```\n\nAfter around 10 trials or rolls of the dice, the probability does a steep decline and stays very low, which would make sense. We can see the actual number by using R.\n\n```{r}\ndbinom(x = 10, size = 20, prob = 1/6)\n```\n\nRolling a \"6\" ten times out of 20 would not be a very probable outcome. You are much more likely to roll one of the other numbers (1 to 5) rather than 6 so many times.\n\n### Flipping a Coin\n\nMost football games and soccer matches start with a coin flip to see who gets the ball first. A coin flip is a great way to think about probabilities. Let's say it's the first game of the season and you are the captain of the soccer team. You are asked to call the coin flip in the air and you call heads. What is the probability of your team starting with the ball?\n\n```{r}\ndbinom(x = 1, size = 1, prob = 0.5)\n```\n\nThis probability was found earlier and it's simply the original formula from a simple coin flip.\n\n$$\np(heads)=\\frac{1}{2}\n$$\n\nLet's imagine that you are the captain of the soccer team and you decide to go with heads every time you call the coin flip. If you got heads, 2, 3, or 4 times in a row, you would probably think you were pretty lucky, but what if you got it 25 times in a row? It would probably be on the nightly news and other teams would start to assume you were cheating. Intuitively, random outcomes seem normal and when someone gets lucky at something, persons begin to take notice. So if the captain of a soccer team picked heads 25 times in a row and got it right every time, that would be a very low probability. Let's take a look:\n\n```{r}\ndbinom(x = 25, size = 25, prob = 0.5)\n```\n\nThat would be a very, very low probability. It's also easy to test. Take a quarter and flip it 25 times. How many times did you get heads? Someone may get it now and again, but not often.\n\n### Graphing the Binomial Distribution\n\nThe binomial distribution is helpful here. Let's graph the distribution of getting heads over 25 trials.\n\n```{r}\nsuccess <- 0:25\nplot(success, dbinom(success, size=25, prob=.5),type='h', \n     col = \"blue\")\n```\n\nDoes this graph remind you of anything? It should because it takes on the shape of a normal curve. Notice that the lowest probabilities are for getting 4 or less heads and 21 or more heads. The highest probabilities are somewhere in the middle, half heads and half tails, which makes sense because the overall probability is 0.5 or $\\frac{1}{2}$. If we increase the number of trials the shape of the distribution is more pronounced and it looks more like a normal curve.\n\n```{r}\nsuccess <- 0:100\nplot(success, dbinom(success, size=100, prob=.5),type='h', col = \"blue\")\n```\n\nSo statistics uses distributions like the binomial distribution to look at the probability of different outcomes, similar to flipping a coin. Flipping a coin and getting heads about half the time is more probable than flipping a coin and getting a larger number of heads or a very small number of heads.\n","srcMarkdownNoYaml":"\n\n## Introduction to Probability\n\nThe foundation of statistics is probability, which is analyzing the chances that an event will or will not occur. Here is the basic formula for probability:\n\n$$ p(A) = \\frac{Number\\ of\\ events\\ classifiable\\ as\\ A}{Total\\ number\\ of\\ possible\\ events} $$\n\nWe can start with a basic example using a deck of 52 playing cards. What's the probability of drawing an Ace?\n\n1.  What are the number of events classifiable as Ace?\n\nA deck of playing cards has 4 aces of the four suits, hearts, diamonds, clubs, and spades. So our numerator is 4.\n\nThe total number of possible events or total number in a deck of cards is 52. So our denominator is 52. So we can write the equation like this: $$ p (Ace)=\\frac{4}{52}$$\n\nWe can use R to find this probability\n\n```{r}\n4/52\n```\n\nProbabilities are always given in proportions or numbers between 0 and 1. If a given probability is 0 it is not possible for a particular event to occur while if a given probability is 1 it is certain that a particular event will occur. For example the probability of getting a 7 on one roll of a 6 sided die is 0 because the only possible outcomes on one roll of a six sided die is between 1 and 6. Whereas when you add together the probabilities of rolling a 1 through 6 together you get 1, because that is the total number of possible outcomes for a six sided die. To demonstrate, the probability of rolling a \"4\" on a six-sided die is one out of six or: $$p(4) = \\frac{1}{6}$$ Or in R\n\n```{r}\n1/6\n```\n\nSo if we add together all the probabilities for each side of the die we'll get 1. $$p(1) = \\frac{1}{6} + p(2) = \\frac{1}{6} + p(3) = \\frac{1}{6} + p(4) = \\frac{1}{6} + p(5) = \\frac{1}{6} + p(6) = \\frac{1}{6} = 1$$ Or in R\n\n```{r}\n1/6+1/6+1/6+1/6+1/6+1/6\n```\n\n### The Rules of Probability\n\nThere are two basic rules to probability, the *addition* rule and the *multiplication* rule. The addition rule applies to the single occurrence of two or more events. For example, what is the probability of drawing an Ace or a King on single draw from a deck of cards. In this case we *add* the probability of drawing an Ace and a King together to find the correct probability. Recall, that the probability for drawing an Ace looks like this:\n\n$$ p (Ace)=\\frac{4}{52}$$\n\nThe probability of drawing a king would be the same as an Ace because there are 4 Kings of each suit in the deck of cards, so the formula looks like this:\n\n$$ p (Ace\\ or\\ a\\ King)=\\frac{4}{52} + \\frac{4}{52}$$\n\nSo the probability of drawing an Ace or a King would be $\\frac{8}{52}$.\n\nUsing R we would find\n\n```{r}\n4/52+4/52\n```\n\nThe *multiplication rule* applies for more than one draw or successive events. For example, what would be the probability of drawing an Ace on the first draw and a King on the second. In that case, you would mutlitply rather than add.\n\n$$ p (Ace\\ and\\ a\\ King)=\\frac{4}{52} \\times \\frac{4}{51}$$\n\nNotice, how in the second fraction the denominator is 51 rather than 52 to account for the fact that after you've drawn the Ace there are only 51 cards left. In this case, the second card is drawn without replacement.\n\n```{r}\n4/52*4/51\n```\n\n### Binomial Distribution\n\nMathematics creates the ability to go beyond probabilities for single events and look at probability distributions or a set of probability values based on a certain number of events. Statistics is based on probability distributions that assign a particular probability to an observed outcome. In psychological science, probability distributions are used to analyze the probability of a particular outcome observed in an experiment. Different probability distributions are used for various statistical tests such as the *t test*, *analysis of variance* or *ANOVA*, and the *chi-square test*.\n\nA good place to begin to understand probability distributions is the binomial distribution. For example, imagine you are flipping an evenly weighted coin. What's the probability of getting heads? Let's go back to our original formula.\n\n-   Number of events classifiable as \"heads\" = 1\n\n-   Total number of possible events = 2\n\nHere's the formula:\n\n$$p(heads) = \\frac{1}{2}$$\n\nWe can use R to get the proportion.\n\n```{r}\n1/2\n```\n\nWhat about the probability of getting 2 heads in a row on 2 flips of the coin? To analyze the probability of successive events or outcomes you need to multiply the probability for each event. This is called the *multiplication rule* for probability. So the formula looks like:\n\n$$p(2heads) = \\frac{1}{2}\\times\\frac{1}{2} = \\frac{1}{4}$$\n\nUsing r we find:\n\n```{r}\n1/2*1/2\n```\n\nSo we could go on and figure out the probability for 3 heads and 4 heads and so on, but instead the binomial distribution can provide a distribution of different probability values based on a given number of events. The binomial distribution is a just an extension of the mathematics we would need to do by hand to find the probability for different events and luckily, as usual, R is able to do this for us.\n\nThe function we'll use is called `dbinom()`. The main arguments for the function are:\n\n`x` The number of outcomes for the given probability currently being calculated\n\n`size` The number of the overall size of the experiment\n\n`prob` The given probability value\n\nSo let's calculate the original probability problem of two heads in two flips of a fair-sided coin. Notice that the answer is the same to the formula we used earlier.\n\n```{r}\ndbinom(x=2, size = 2, prob = 1/2)\n```\n\nProbabilities can be listed as fractions or proportions\n\n```{r}\ndbinom(x=2, size = 2, prob = 0.5)\n```\n\nWe can also use it for other probability values such as rolling a six sided die, which has the probability value of $\\frac{1}{6}$\n\n```{r}\ndbinom(x=1, size = 1, prob = 1/6)\n```\n\nOf course, it's most helpful for probabilities for larger numbers of events. Like what's the probability of rolling 4 sixes over 20 trials.\n\n```{r}\ndbinom(x =4, size = 20, prob = 1/6)\n```\n\nWe can also represent this as distribution graph.\n\n```{r}\nsuccess <- 0:20\nplot(success, dbinom(success, size=20, prob=1/6),type='h',\n     col = \"blue\")\n```\n\nEach of the lines represents the probability for a given outcome. Notice how the outcome for rolling 4 sixes in 20 trials is around 0.20, which was found in the original calculation using the `dbinom()` function. The highest probability is to roll a \"6\" three times out of 20 rolls of the dice. Notice how the graph of the binomial distribution shows us the same answer as using the `dbinom()` function.\n\n```{r}\ndbinom(x=3, size = 20, prob = 1/6)\n```\n\nAfter around 10 trials or rolls of the dice, the probability does a steep decline and stays very low, which would make sense. We can see the actual number by using R.\n\n```{r}\ndbinom(x = 10, size = 20, prob = 1/6)\n```\n\nRolling a \"6\" ten times out of 20 would not be a very probable outcome. You are much more likely to roll one of the other numbers (1 to 5) rather than 6 so many times.\n\n### Flipping a Coin\n\nMost football games and soccer matches start with a coin flip to see who gets the ball first. A coin flip is a great way to think about probabilities. Let's say it's the first game of the season and you are the captain of the soccer team. You are asked to call the coin flip in the air and you call heads. What is the probability of your team starting with the ball?\n\n```{r}\ndbinom(x = 1, size = 1, prob = 0.5)\n```\n\nThis probability was found earlier and it's simply the original formula from a simple coin flip.\n\n$$\np(heads)=\\frac{1}{2}\n$$\n\nLet's imagine that you are the captain of the soccer team and you decide to go with heads every time you call the coin flip. If you got heads, 2, 3, or 4 times in a row, you would probably think you were pretty lucky, but what if you got it 25 times in a row? It would probably be on the nightly news and other teams would start to assume you were cheating. Intuitively, random outcomes seem normal and when someone gets lucky at something, persons begin to take notice. So if the captain of a soccer team picked heads 25 times in a row and got it right every time, that would be a very low probability. Let's take a look:\n\n```{r}\ndbinom(x = 25, size = 25, prob = 0.5)\n```\n\nThat would be a very, very low probability. It's also easy to test. Take a quarter and flip it 25 times. How many times did you get heads? Someone may get it now and again, but not often.\n\n### Graphing the Binomial Distribution\n\nThe binomial distribution is helpful here. Let's graph the distribution of getting heads over 25 trials.\n\n```{r}\nsuccess <- 0:25\nplot(success, dbinom(success, size=25, prob=.5),type='h', \n     col = \"blue\")\n```\n\nDoes this graph remind you of anything? It should because it takes on the shape of a normal curve. Notice that the lowest probabilities are for getting 4 or less heads and 21 or more heads. The highest probabilities are somewhere in the middle, half heads and half tails, which makes sense because the overall probability is 0.5 or $\\frac{1}{2}$. If we increase the number of trials the shape of the distribution is more pronounced and it looks more like a normal curve.\n\n```{r}\nsuccess <- 0:100\nplot(success, dbinom(success, size=100, prob=.5),type='h', col = \"blue\")\n```\n\nSo statistics uses distributions like the binomial distribution to look at the probability of different outcomes, similar to flipping a coin. Flipping a coin and getting heads about half the time is more probable than flipping a coin and getting a larger number of heads or a very small number of heads.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"Probability Theory.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","editor":"visual","theme":{"light":"minty","dark":"darkly"},"title":"Probability Theory","author":"James Van Slyke"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}