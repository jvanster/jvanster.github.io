[
  {
    "objectID": "Intro to ggplot.html",
    "href": "Intro to ggplot.html",
    "title": "Intro to ggplot",
    "section": "",
    "text": "ggplot is a graphics language that we use to make graphs. The text we will use for our first go at making graphs is called R for Data Science\nLike a lot of other attractive things about R and R Studio the book is free!! and probably one of the best resources for understanding R and Data.\nYou can find the book here\nI’ll be using several modified examples from Chapter 3 of that book.",
    "crumbs": [
      "Graphs",
      "Intro to ggplot"
    ]
  },
  {
    "objectID": "Intro to ggplot.html#using-ggplot",
    "href": "Intro to ggplot.html#using-ggplot",
    "title": "Intro to ggplot",
    "section": "",
    "text": "ggplot is a graphics language that we use to make graphs. The text we will use for our first go at making graphs is called R for Data Science\nLike a lot of other attractive things about R and R Studio the book is free!! and probably one of the best resources for understanding R and Data.\nYou can find the book here\nI’ll be using several modified examples from Chapter 3 of that book.",
    "crumbs": [
      "Graphs",
      "Intro to ggplot"
    ]
  },
  {
    "objectID": "Intro to ggplot.html#the-tidyverse",
    "href": "Intro to ggplot.html#the-tidyverse",
    "title": "Intro to ggplot",
    "section": "The Tidyverse",
    "text": "The Tidyverse\nTidyverse is a package we’ll be using throughout our exploration of statistics. There are lots of great helps for doing data science.\nFirst you’ll want to load “tidyverse” as a package. Here’s how we find and load packages in R Studio.\n\nSearch under “Packages” in the bottom right window\nClick on install and search for tidyverse in the CRAN repository\nOnce you’ve found tidyverse, click on install - It may take some time for the package to download and install - wait until it is all done.\nOnce the package is downloaded just make sure to check next to the package on your list of packages OR simply use this code.\n\nlibrary(tidyverse)\n\n\n\nmpg dataset\nWe’ll start off by using a dataset called “mpg” It should have loaded with the tidyverse package. Just type it in to find it\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nIt shows the dataset as a “tibble”, which just means table in the tidyverse\nYou can also view it as a spreadsheet in a new window\n\nView(mpg)\n\nBecause it’s a preloaded dataset we can get information about it by using a question mark. A question mark in front of any command in R will automatically generate the help file\n\n?mpg",
    "crumbs": [
      "Graphs",
      "Intro to ggplot"
    ]
  },
  {
    "objectID": "Intro to ggplot.html#levels-of-measurement",
    "href": "Intro to ggplot.html#levels-of-measurement",
    "title": "Intro to ggplot",
    "section": "Levels of Measurement",
    "text": "Levels of Measurement\n\nCategorical\n\nBinary variable\nNominal variable\nOrdinal variable\n\nContinuous\n\nInterval variable\nRatio variable\n\n\nDatasets contain several different variables. Notice in the mpg dataset, there are several different variables such as model, year, cty, and hwy. The str command shows all the different variables.\n\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n\nThese are several different types of variables because they represent different kinds of things. In statistics we refer to this as levels of measurement. There are two broad types of variables, categorical and continuous (Field, 2009, pages 8-9). Categorical variables are based on particular categories, such a type of shoe, religious affiliation, or political affiliation. A binary variable is a type of categorical variable that only takes two categories. So things like being pregnant or not, voting yes or no on a certain bill, or being alive or dead are binary variables, there are only two possible things in the category. Categorical variables that take on more than 2 possibilities are called nominal variables (nominal means names)\n\nCategorical\nThere’s no mathematics involved in determining categorical variables, it’s simply based on which category has the most accurate fit. Either you are a republican, democrat, or independent, there is no mathematical quantity that would determine the appropriate category. There is one type of categorical variable that is ordered based on the absence or presence of a particular property, which is an ordinal variable. Ordinal variables have a particular rank order, but the rankings are not equal or uniform. So for example, college basketball team rankings would be an ordinal variable. The teams are ranked from best to worst, but team #2 may be twice is good as team #3, while team #5 maybe four times as good as team #6. So ordinal data tells us more about the variable than nominal data, but it still doesn’t provide a standardized scale of measurement.\n\n\nContinuous\nContinuous variables are the second broad category and involves some type of numerical measurement to define the property in the variable. Continuous variables can take on any value in the measurement scale defined by the variable. The first example of this type variable is an interval variable. These types of variables are based on a measurement scale with equal distances between the ranks based on the property measured; equal intervals in the scale are able to represent equal differences in the variable being measured. The Fahrenheit scale would be an interval scale because the difference in degree between 64 and 65 is the same as 74 and 75.\n\n\nRatio\nRatio variables add one more dimension to the properties of an interval variable. In addition to having an equal distance in ranks, ratios have an absolute zero point. This allows for multiplication of the intervals or the use of ratios. So on a ratio scale of 0 to 5, a score of 4 would be twice as good as a score of 2. Time is a good example of a ratio scale. There is an absolute zero point (there is no such thing as negative time), the distance between 40 and 60 seconds is the same as the distance between 30 and 10 seconds. And 20 seconds is twice as long as 10 seconds. Another example, Fahrenheit is an interval scale because you can have negative degrees (e. g. -2 degrees below zero), while the Kelvin scale is a ratio scale because there is an absolute zero point and no negative numbers.\nOne other property of continuous variables is that they allow for different degrees of measurement precision. So for example, the continuous ratio variable of time can be measured in hours, minutes, seconds, milliseconds, etc. In contrast, a discrete variable can only take on a fixed measurement scale, like a rank scale of 1 to 10. The scale requires that you choose a value between 1 and 10, 2.5 is not possible.",
    "crumbs": [
      "Graphs",
      "Intro to ggplot"
    ]
  },
  {
    "objectID": "Intro to ggplot.html#scatterplot",
    "href": "Intro to ggplot.html#scatterplot",
    "title": "Intro to ggplot",
    "section": "Scatterplot",
    "text": "Scatterplot\nLet’s start by creating a simple scatterplot graph\n\nggplot(data = mpg) +\n  geom_point(mapping = aes(x= displ, y=hwy))\n\n\n\n\n\n\n\n\nThere’s a few things to learn here in the code\n\nggplot is the basic command and within the parentheses is the data we’ll be running our graph on\ngeom_point is the type of graph will be using, geom stands for geometrical shape. So in this case we are making a graph of “points”\nThe “mapping” argument lays out the variables we are graphing and is always paired with “aes”. Finally x and y lay out which variables are going to be on our x and y axes.\n\nSo our basic formula looks lke this:\nggplot(data = ) + (mapping = aes())\n\nAbout the variables\nIf you look at the variables, you’ll notice that hwy and displ are numbers and R views them as integers or whole numbers. Scatterplots usually require data that are numeric like this, numbers that go up in scale. So in this case scatterplots typically require interval or ratio data.\nOne peculiar thing about this graph is the group of numbers just above the numbers in the right corner. The general trend we see in these variables is that as displacement goes up (i.e. the engine uses more gasoline) the highway mileage goes down. This is sometimes referred to as a negative relationship. The cars with the best highway mileage displace the least amount of gasoline. However, there’s a group of dots that range between 20 and 30 on the hwy variable, but seem to displace more gasoline than most. How can this be?\n\n\nAdding additional variables\nOne way we can answer this question is by adding in an additional variable. In this case the class or type of car. Let’s add that in now as an additional variable to our graph.\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\n\n\nNotice how most of those dots are 2 seater cars. Thus, the reason they get better gas mileage is because they are smaller cars!!\nClass is a particular kind of variable, in this case a nominal or categorical variable. r refers to them as characters. These are basically categories, so we can’t do any math on them. You can’t make a formula out of compact x midsize or minivan/pickup.\n\n\nAdd Color\nLet’s do one more graph, but add a little color\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")\n\n\n\n\n\n\n\n\nNotice how when the word “color” is outside the x and y mappings it changes the color of our points. You can try several different colors for the dots on your scatterplot.",
    "crumbs": [
      "Graphs",
      "Intro to ggplot"
    ]
  },
  {
    "objectID": "Intro to ggplot.html#bar-graphs",
    "href": "Intro to ggplot.html#bar-graphs",
    "title": "Intro to ggplot",
    "section": "Bar Graphs",
    "text": "Bar Graphs\nLet’s learn one more graph, which is especially important for categorical variables, the bar graph.\nFirst, check out the diamonds data set\n\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nThe basics of the formula are going to be the same, but this time we are using geom_bar as our geometrical shape to create a bar graph.\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut))\n\n\n\n\n\n\n\n\nIf you look in section 3.7 in R for Data Science it explains what R is doing with the data\n\nSo R looks at the data set and automatically uses the count statistic to simply count the number of occurrences for that variable. So this only works for categorical variables.\nLet’s inspect the cut variable to see if this is true. Remember that the str command allows us to investigate the types of variables in the dataset.\n\nstr(diamonds)\n\ntibble [53,940 × 10] (S3: tbl_df/tbl/data.frame)\n $ carat  : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ...\n $ cut    : Ord.factor w/ 5 levels \"Fair\"&lt;\"Good\"&lt;..: 5 4 2 4 2 3 3 3 1 3 ...\n $ color  : Ord.factor w/ 7 levels \"D\"&lt;\"E\"&lt;\"F\"&lt;\"G\"&lt;..: 2 2 2 6 7 7 6 5 2 5 ...\n $ clarity: Ord.factor w/ 8 levels \"I1\"&lt;\"SI2\"&lt;\"SI1\"&lt;..: 2 3 5 4 2 6 7 3 4 5 ...\n $ depth  : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ...\n $ table  : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ...\n $ price  : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ...\n $ x      : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ...\n $ y      : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ...\n $ z      : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ...\n\n\nEach of the variables are preceded by the $ sign. So we can see that cut is the second variable. Notice that it says cut is an ord.factor with 5 levels. This tells us that this variable is an ordinal variable because it is ordered based on the type of cut from fair (lowest type of cut) to ideal (the best type of cut).\n\nAnother example\nHere’s another example from scratch. First I’ll create a quick dataset using the tribble command.\n\nExample &lt;- tribble(\n  ~group, ~number, \n  \"Group 1\", 30, \n  \"Group 2\", 50\n)\n\nNotice that our variable group is a categorical variable, but on it’s own the stat count won’t really tell us much (Group 1 = 1, Group 2 =1). So notice what happens when we try to make a bar graph like our last example.\n\nggplot(data = Example) + \n  geom_bar(mapping = aes(x = group))\n\n\n\n\n\n\n\n\nDoesn’t really tell us much does it? So in this case the count has to be supplied by a second variable, y. When we use a y variable for our bar chart we have to use a different stat for the bar chart, in this case, the stat is “identity” so the code looks like this.\n\nggplot(data = Example) +\n  geom_bar(mapping = aes(x = group, y = number), stat = \"identity\")\n\n\n\n\n\n\n\n\nNow it’s easy to see the difference in count between Group 1 and Group 2",
    "crumbs": [
      "Graphs",
      "Intro to ggplot"
    ]
  },
  {
    "objectID": "Probability Theory.html",
    "href": "Probability Theory.html",
    "title": "Probability Theory",
    "section": "",
    "text": "The foundation of statistics is probability, which is analyzing the chances that an event will or will not occur. Here is the basic formula for probability:\n\\[ p(A) = \\frac{Number\\ of\\ events\\ classifiable\\ as\\ A}{Total\\ number\\ of\\ possible\\ events} \\]\nWe can start with a basic example using a deck of 52 playing cards. What’s the probability of drawing an Ace?\n\nWhat are the number of events classifiable as Ace?\n\nA deck of playing cards has 4 aces of the four suits, hearts, diamonds, clubs, and spades. So our numerator is 4.\nThe total number of possible events or total number in a deck of cards is 52. So our denominator is 52. So we can write the equation like this: \\[ p (Ace)=\\frac{4}{52}\\]\nWe can use R to find this probability\n\n4/52\n\n[1] 0.07692308\n\n\nProbabilities are always given in proportions or numbers between 0 and 1. If a given probability is 0 it is not possible for a particular event to occur while if a given probability is 1 it is certain that a particular event will occur. For example the probability of getting a 7 on one roll of a 6 sided die is 0 because the only possible outcomes on one roll of a six sided die is between 1 and 6. Whereas when you add together the probabilities of rolling a 1 through 6 together you get 1, because that is the total number of possible outcomes for a six sided die. To demonstrate, the probability of rolling a “4” on a six-sided die is one out of six or: \\[p(4) = \\frac{1}{6}\\] Or in R\n\n1/6\n\n[1] 0.1666667\n\n\nSo if we add together all the probabilities for each side of the die we’ll get 1. \\[p(1) = \\frac{1}{6} + p(2) = \\frac{1}{6} + p(3) = \\frac{1}{6} + p(4) = \\frac{1}{6} + p(5) = \\frac{1}{6} + p(6) = \\frac{1}{6} = 1\\] Or in R\n\n1/6+1/6+1/6+1/6+1/6+1/6\n\n[1] 1\n\n\n\n\nThere are two basic rules to probability, the addition rule and the multiplication rule. The addition rule applies to the single occurrence of two or more events. For example, what is the probability of drawing an Ace or a King on single draw from a deck of cards. In this case we add the probability of drawing an Ace and a King together to find the correct probability. Recall, that the probability for drawing an Ace looks like this:\n\\[ p (Ace)=\\frac{4}{52}\\]\nThe probability of drawing a king would be the same as an Ace because there are 4 Kings of each suit in the deck of cards, so the formula looks like this:\n\\[ p (Ace\\ or\\ a\\ King)=\\frac{4}{52} + \\frac{4}{52}\\]\nSo the probability of drawing an Ace or a King would be \\(\\frac{8}{52}\\).\nUsing R we would find\n\n4/52+4/52\n\n[1] 0.1538462\n\n\nThe multiplication rule applies for more than one draw or successive events. For example, what would be the probability of drawing an Ace on the first draw and a King on the second. In that case, you would mutlitply rather than add.\n\\[ p (Ace\\ and\\ a\\ King)=\\frac{4}{52} \\times \\frac{4}{51}\\]\nNotice, how in the second fraction the denominator is 51 rather than 52 to account for the fact that after you’ve drawn the Ace there are only 51 cards left. In this case, the second card is drawn without replacement.\n\n4/52*4/51\n\n[1] 0.006033183\n\n\n\n\n\nMathematics creates the ability to go beyond probabilities for single events and look at probability distributions or a set of probability values based on a certain number of events. Statistics is based on probability distributions that assign a particular probability to an observed outcome. In psychological science, probability distributions are used to analyze the probability of a particular outcome observed in an experiment. Different probability distributions are used for various statistical tests such as the t test, analysis of variance or ANOVA, and the chi-square test.\nA good place to begin to understand probability distributions is the binomial distribution. For example, imagine you are flipping an evenly weighted coin. What’s the probability of getting heads? Let’s go back to our original formula.\n\nNumber of events classifiable as “heads” = 1\nTotal number of possible events = 2\n\nHere’s the formula:\n\\[p(heads) = \\frac{1}{2}\\]\nWe can use R to get the proportion.\n\n1/2\n\n[1] 0.5\n\n\nWhat about the probability of getting 2 heads in a row on 2 flips of the coin? To analyze the probability of successive events or outcomes you need to multiply the probability for each event. This is called the multiplication rule for probability. So the formula looks like:\n\\[p(2heads) = \\frac{1}{2}\\times\\frac{1}{2} = \\frac{1}{4}\\]\nUsing r we find:\n\n1/2*1/2\n\n[1] 0.25\n\n\nSo we could go on and figure out the probability for 3 heads and 4 heads and so on, but instead the binomial distribution can provide a distribution of different probability values based on a given number of events. The binomial distribution is a just an extension of the mathematics we would need to do by hand to find the probability for different events and luckily, as usual, R is able to do this for us.\nThe function we’ll use is called dbinom(). The main arguments for the function are:\nx The number of outcomes for the given probability currently being calculated\nsize The number of the overall size of the experiment\nprob The given probability value\nSo let’s calculate the original probability problem of two heads in two flips of a fair-sided coin. Notice that the answer is the same to the formula we used earlier.\n\ndbinom(x=2, size = 2, prob = 1/2)\n\n[1] 0.25\n\n\nProbabilities can be listed as fractions or proportions\n\ndbinom(x=2, size = 2, prob = 0.5)\n\n[1] 0.25\n\n\nWe can also use it for other probability values such as rolling a six sided die, which has the probability value of \\(\\frac{1}{6}\\)\n\ndbinom(x=1, size = 1, prob = 1/6)\n\n[1] 0.1666667\n\n\nOf course, it’s most helpful for probabilities for larger numbers of events. Like what’s the probability of rolling 4 sixes over 20 trials.\n\ndbinom(x =4, size = 20, prob = 1/6)\n\n[1] 0.2022036\n\n\nWe can also represent this as distribution graph.\n\nsuccess &lt;- 0:20\nplot(success, dbinom(success, size=20, prob=1/6),type='h',\n     col = \"blue\")\n\n\n\n\n\n\n\n\nEach of the lines represents the probability for a given outcome. Notice how the outcome for rolling 4 sixes in 20 trials is around 0.20, which was found in the original calculation using the dbinom() function. The highest probability is to roll a “6” three times out of 20 rolls of the dice. Notice how the graph of the binomial distribution shows us the same answer as using the dbinom() function.\n\ndbinom(x=3, size = 20, prob = 1/6)\n\n[1] 0.2378866\n\n\nAfter around 10 trials or rolls of the dice, the probability does a steep decline and stays very low, which would make sense. We can see the actual number by using R.\n\ndbinom(x = 10, size = 20, prob = 1/6)\n\n[1] 0.0004934846\n\n\nRolling a “6” ten times out of 20 would not be a very probable outcome. You are much more likely to roll one of the other numbers (1 to 5) rather than 6 so many times.\n\n\n\nMost football games and soccer matches start with a coin flip to see who gets the ball first. A coin flip is a great way to think about probabilities. Let’s say it’s the first game of the season and you are the captain of the soccer team. You are asked to call the coin flip in the air and you call heads. What is the probability of your team starting with the ball?\n\ndbinom(x = 1, size = 1, prob = 0.5)\n\n[1] 0.5\n\n\nThis probability was found earlier and it’s simply the original formula from a simple coin flip.\n\\[\np(heads)=\\frac{1}{2}\n\\]\nLet’s imagine that you are the captain of the soccer team and you decide to go with heads every time you call the coin flip. If you got heads, 2, 3, or 4 times in a row, you would probably think you were pretty lucky, but what if you got it 25 times in a row? It would probably be on the nightly news and other teams would start to assume you were cheating. Intuitively, random outcomes seem normal and when someone gets lucky at something, persons begin to take notice. So if the captain of a soccer team picked heads 25 times in a row and got it right every time, that would be a very low probability. Let’s take a look:\n\ndbinom(x = 25, size = 25, prob = 0.5)\n\n[1] 2.980232e-08\n\n\nThat would be a very, very low probability. It’s also easy to test. Take a quarter and flip it 25 times. How many times did you get heads? Someone may get it now and again, but not often.\n\n\n\nThe binomial distribution is helpful here. Let’s graph the distribution of getting heads over 25 trials.\n\nsuccess &lt;- 0:25\nplot(success, dbinom(success, size=25, prob=.5),type='h', \n     col = \"blue\")\n\n\n\n\n\n\n\n\nDoes this graph remind you of anything? It should because it takes on the shape of a normal curve. Notice that the lowest probabilities are for getting 4 or less heads and 21 or more heads. The highest probabilities are somewhere in the middle, half heads and half tails, which makes sense because the overall probability is 0.5 or \\(\\frac{1}{2}\\). If we increase the number of trials the shape of the distribution is more pronounced and it looks more like a normal curve.\n\nsuccess &lt;- 0:100\nplot(success, dbinom(success, size=100, prob=.5),type='h', col = \"blue\")\n\n\n\n\n\n\n\n\nSo statistics uses distributions like the binomial distribution to look at the probability of different outcomes, similar to flipping a coin. Flipping a coin and getting heads about half the time is more probable than flipping a coin and getting a larger number of heads or a very small number of heads.",
    "crumbs": [
      "Intro to Statistics",
      "Probability Theory"
    ]
  },
  {
    "objectID": "Probability Theory.html#introduction-to-probability",
    "href": "Probability Theory.html#introduction-to-probability",
    "title": "Probability Theory",
    "section": "",
    "text": "The foundation of statistics is probability, which is analyzing the chances that an event will or will not occur. Here is the basic formula for probability:\n\\[ p(A) = \\frac{Number\\ of\\ events\\ classifiable\\ as\\ A}{Total\\ number\\ of\\ possible\\ events} \\]\nWe can start with a basic example using a deck of 52 playing cards. What’s the probability of drawing an Ace?\n\nWhat are the number of events classifiable as Ace?\n\nA deck of playing cards has 4 aces of the four suits, hearts, diamonds, clubs, and spades. So our numerator is 4.\nThe total number of possible events or total number in a deck of cards is 52. So our denominator is 52. So we can write the equation like this: \\[ p (Ace)=\\frac{4}{52}\\]\nWe can use R to find this probability\n\n4/52\n\n[1] 0.07692308\n\n\nProbabilities are always given in proportions or numbers between 0 and 1. If a given probability is 0 it is not possible for a particular event to occur while if a given probability is 1 it is certain that a particular event will occur. For example the probability of getting a 7 on one roll of a 6 sided die is 0 because the only possible outcomes on one roll of a six sided die is between 1 and 6. Whereas when you add together the probabilities of rolling a 1 through 6 together you get 1, because that is the total number of possible outcomes for a six sided die. To demonstrate, the probability of rolling a “4” on a six-sided die is one out of six or: \\[p(4) = \\frac{1}{6}\\] Or in R\n\n1/6\n\n[1] 0.1666667\n\n\nSo if we add together all the probabilities for each side of the die we’ll get 1. \\[p(1) = \\frac{1}{6} + p(2) = \\frac{1}{6} + p(3) = \\frac{1}{6} + p(4) = \\frac{1}{6} + p(5) = \\frac{1}{6} + p(6) = \\frac{1}{6} = 1\\] Or in R\n\n1/6+1/6+1/6+1/6+1/6+1/6\n\n[1] 1\n\n\n\n\nThere are two basic rules to probability, the addition rule and the multiplication rule. The addition rule applies to the single occurrence of two or more events. For example, what is the probability of drawing an Ace or a King on single draw from a deck of cards. In this case we add the probability of drawing an Ace and a King together to find the correct probability. Recall, that the probability for drawing an Ace looks like this:\n\\[ p (Ace)=\\frac{4}{52}\\]\nThe probability of drawing a king would be the same as an Ace because there are 4 Kings of each suit in the deck of cards, so the formula looks like this:\n\\[ p (Ace\\ or\\ a\\ King)=\\frac{4}{52} + \\frac{4}{52}\\]\nSo the probability of drawing an Ace or a King would be \\(\\frac{8}{52}\\).\nUsing R we would find\n\n4/52+4/52\n\n[1] 0.1538462\n\n\nThe multiplication rule applies for more than one draw or successive events. For example, what would be the probability of drawing an Ace on the first draw and a King on the second. In that case, you would mutlitply rather than add.\n\\[ p (Ace\\ and\\ a\\ King)=\\frac{4}{52} \\times \\frac{4}{51}\\]\nNotice, how in the second fraction the denominator is 51 rather than 52 to account for the fact that after you’ve drawn the Ace there are only 51 cards left. In this case, the second card is drawn without replacement.\n\n4/52*4/51\n\n[1] 0.006033183\n\n\n\n\n\nMathematics creates the ability to go beyond probabilities for single events and look at probability distributions or a set of probability values based on a certain number of events. Statistics is based on probability distributions that assign a particular probability to an observed outcome. In psychological science, probability distributions are used to analyze the probability of a particular outcome observed in an experiment. Different probability distributions are used for various statistical tests such as the t test, analysis of variance or ANOVA, and the chi-square test.\nA good place to begin to understand probability distributions is the binomial distribution. For example, imagine you are flipping an evenly weighted coin. What’s the probability of getting heads? Let’s go back to our original formula.\n\nNumber of events classifiable as “heads” = 1\nTotal number of possible events = 2\n\nHere’s the formula:\n\\[p(heads) = \\frac{1}{2}\\]\nWe can use R to get the proportion.\n\n1/2\n\n[1] 0.5\n\n\nWhat about the probability of getting 2 heads in a row on 2 flips of the coin? To analyze the probability of successive events or outcomes you need to multiply the probability for each event. This is called the multiplication rule for probability. So the formula looks like:\n\\[p(2heads) = \\frac{1}{2}\\times\\frac{1}{2} = \\frac{1}{4}\\]\nUsing r we find:\n\n1/2*1/2\n\n[1] 0.25\n\n\nSo we could go on and figure out the probability for 3 heads and 4 heads and so on, but instead the binomial distribution can provide a distribution of different probability values based on a given number of events. The binomial distribution is a just an extension of the mathematics we would need to do by hand to find the probability for different events and luckily, as usual, R is able to do this for us.\nThe function we’ll use is called dbinom(). The main arguments for the function are:\nx The number of outcomes for the given probability currently being calculated\nsize The number of the overall size of the experiment\nprob The given probability value\nSo let’s calculate the original probability problem of two heads in two flips of a fair-sided coin. Notice that the answer is the same to the formula we used earlier.\n\ndbinom(x=2, size = 2, prob = 1/2)\n\n[1] 0.25\n\n\nProbabilities can be listed as fractions or proportions\n\ndbinom(x=2, size = 2, prob = 0.5)\n\n[1] 0.25\n\n\nWe can also use it for other probability values such as rolling a six sided die, which has the probability value of \\(\\frac{1}{6}\\)\n\ndbinom(x=1, size = 1, prob = 1/6)\n\n[1] 0.1666667\n\n\nOf course, it’s most helpful for probabilities for larger numbers of events. Like what’s the probability of rolling 4 sixes over 20 trials.\n\ndbinom(x =4, size = 20, prob = 1/6)\n\n[1] 0.2022036\n\n\nWe can also represent this as distribution graph.\n\nsuccess &lt;- 0:20\nplot(success, dbinom(success, size=20, prob=1/6),type='h',\n     col = \"blue\")\n\n\n\n\n\n\n\n\nEach of the lines represents the probability for a given outcome. Notice how the outcome for rolling 4 sixes in 20 trials is around 0.20, which was found in the original calculation using the dbinom() function. The highest probability is to roll a “6” three times out of 20 rolls of the dice. Notice how the graph of the binomial distribution shows us the same answer as using the dbinom() function.\n\ndbinom(x=3, size = 20, prob = 1/6)\n\n[1] 0.2378866\n\n\nAfter around 10 trials or rolls of the dice, the probability does a steep decline and stays very low, which would make sense. We can see the actual number by using R.\n\ndbinom(x = 10, size = 20, prob = 1/6)\n\n[1] 0.0004934846\n\n\nRolling a “6” ten times out of 20 would not be a very probable outcome. You are much more likely to roll one of the other numbers (1 to 5) rather than 6 so many times.\n\n\n\nMost football games and soccer matches start with a coin flip to see who gets the ball first. A coin flip is a great way to think about probabilities. Let’s say it’s the first game of the season and you are the captain of the soccer team. You are asked to call the coin flip in the air and you call heads. What is the probability of your team starting with the ball?\n\ndbinom(x = 1, size = 1, prob = 0.5)\n\n[1] 0.5\n\n\nThis probability was found earlier and it’s simply the original formula from a simple coin flip.\n\\[\np(heads)=\\frac{1}{2}\n\\]\nLet’s imagine that you are the captain of the soccer team and you decide to go with heads every time you call the coin flip. If you got heads, 2, 3, or 4 times in a row, you would probably think you were pretty lucky, but what if you got it 25 times in a row? It would probably be on the nightly news and other teams would start to assume you were cheating. Intuitively, random outcomes seem normal and when someone gets lucky at something, persons begin to take notice. So if the captain of a soccer team picked heads 25 times in a row and got it right every time, that would be a very low probability. Let’s take a look:\n\ndbinom(x = 25, size = 25, prob = 0.5)\n\n[1] 2.980232e-08\n\n\nThat would be a very, very low probability. It’s also easy to test. Take a quarter and flip it 25 times. How many times did you get heads? Someone may get it now and again, but not often.\n\n\n\nThe binomial distribution is helpful here. Let’s graph the distribution of getting heads over 25 trials.\n\nsuccess &lt;- 0:25\nplot(success, dbinom(success, size=25, prob=.5),type='h', \n     col = \"blue\")\n\n\n\n\n\n\n\n\nDoes this graph remind you of anything? It should because it takes on the shape of a normal curve. Notice that the lowest probabilities are for getting 4 or less heads and 21 or more heads. The highest probabilities are somewhere in the middle, half heads and half tails, which makes sense because the overall probability is 0.5 or \\(\\frac{1}{2}\\). If we increase the number of trials the shape of the distribution is more pronounced and it looks more like a normal curve.\n\nsuccess &lt;- 0:100\nplot(success, dbinom(success, size=100, prob=.5),type='h', col = \"blue\")\n\n\n\n\n\n\n\n\nSo statistics uses distributions like the binomial distribution to look at the probability of different outcomes, similar to flipping a coin. Flipping a coin and getting heads about half the time is more probable than flipping a coin and getting a larger number of heads or a very small number of heads.",
    "crumbs": [
      "Intro to Statistics",
      "Probability Theory"
    ]
  },
  {
    "objectID": "Pres Trial.html",
    "href": "Pres Trial.html",
    "title": "Presentation Trial",
    "section": "",
    "text": "Turn off alarm\nGet out of bed\nEat spaghetti"
  },
  {
    "objectID": "Pres Trial.html#slide-1",
    "href": "Pres Trial.html#slide-1",
    "title": "Presentation Trial",
    "section": "",
    "text": "Turn off alarm\nGet out of bed\nEat spaghetti"
  },
  {
    "objectID": "Pres Trial.html#slide-2",
    "href": "Pres Trial.html#slide-2",
    "title": "Presentation Trial",
    "section": "Slide 2",
    "text": "Slide 2\n\nGet in bed\nCount sheep\n\n\nAnother Slide"
  },
  {
    "objectID": "Why R.html",
    "href": "Why R.html",
    "title": "Why R?",
    "section": "",
    "text": "Why R & R Studio?\nMany persons are curious about the shift to using R rather than SPSS or one of the other basic computer programs. A great overview is offered here:\nSPSS is dying\n\n\nReasons to Move to R Studio\nHere are the reasons I think it’s important to move to R Studio.\n\n\n1. It’s free.\nYou can put it on your home computer for free. You can use it on Windows and Macs and even inexpensive educational computers like Raspberry pi\nRaspberry pi\nIf you learn SPSS, most likely your company won’t have it and your educational organization may or may not have it. If you learn R and R Studio you can take it into any work setting and use it. This is especially helpful for nonprofits who often can’t afford statistical software.\n\n\n2. There’s this new thing called a ‘com-pu-tor’\nComputers are ubiquitous, you probably carried one with you into class in your pocket. A contemporary iPhone is 120 million times more powerful than the computers that sent humans to the moon. However, not everyone is as familiar with the various types of code that makes modern computers, iPhones, apps, Instagram, etc., run. Learning R gives you a very rudimentary understanding of how to code. More importantly, it teaches you how to manipulate code. No one ever writes code on their own from scratch, but parts are taken and then transformed to work for your specific needs. The introduction to r and code in this class allows you to do basic statistics, make professional looking graphs, and even make websites (This website was created using R Studio and Quarto).\n\n\n3. Simplifying replication\nA big story in the last couple of years is the problem with replication in the social sciences, some calling it a replication crisis. Replication is also requiring access to the code you used, not just the output shared in the research article. Mistakes have been found with scientific research findings because of the computations performed and how the data was processed, which haven’t been a normal part of research reports in the past. R Studio makes this easier because as you learn to code, you can also share the code you used to give a complete picture of the data analysis process from start to finish. Many journals are beginning to require that authors include their data as well as their code in the submission of research manuscripts.\n\n\n4. Collaboration\nWebsites like GitHub are making it more and more easier to collaborate on various projects together. Github enables users to monitor how data is manipulated and changed over time. R Studio helps to facilitate this collaboration. More and more websites and companies are making their data free and open source (just like R is) so that persons can learn from each other or look at the data for themselves and check the sources. For example, here’s a site that actively pulls data from The NY Times data repository on Covid-19 to be used in R.\n\n\n5. Self-Teaching\nMost of the resources you need to learn R and R Studio are free. I taught myself R Studio through using free open source books, websites, and YouTube videos. Any time I had a question, I just googled it! I could usually find the answer with a little work. R also includes different packages that allow you to do different types of tasks (we use tidyverse for our class and ggplot) These are free as well and have tons of online help for free training. If you start with a little bit of R Studio, you can literally go anywhere in the world of data science and analysis.\n\n\n6. Social Justice\nR Studio has committed to donate time and resources to various social justice groups such as Black Lives Matter and the ACLU\nAs a company, R Studio is committed to providing open-source free software for data science (they also have an enterprise wing that provides data services for a fee). They are also a Designated Public Benefit Corporation, which means that their open source mission is written into their charter and stakeholders must uphold that mission in their decision-making\nRStudio was recently renamed Posit and you can see their annual report here",
    "crumbs": [
      "R Basics",
      "Why R?"
    ]
  },
  {
    "objectID": "Intro to R.html",
    "href": "Intro to R.html",
    "title": "Intro to R & R Studio",
    "section": "",
    "text": "R is the base program for R Studio, it does all the calculations, while R studio is the addition of several windows around R that helps with your analysis\nIn R Studio, “R” performs calculations in the console. The console is in the lower left window. You can think of the console as a big calculator. You can do all the basics with R.\nThe Editor is where you write the code and notes to yourself in scripts. The console is where you code gets entered and run and the output is in the lower right hand corner. The upper left hand window is the environment and keeps track of the data and other things used in your analysis.\nHere is a picture of what the program looks like and the different sections.",
    "crumbs": [
      "R Basics",
      "Intro to R & R Studio"
    ]
  },
  {
    "objectID": "Intro to R.html#vectors",
    "href": "Intro to R.html#vectors",
    "title": "Intro to R & R Studio",
    "section": "Vectors",
    "text": "Vectors\nBesides individual numbers your can also create vectors or arrays of numbers, which are a set of number that are saved in an object\nHere is a basic vector of numbers\n\nVector &lt;- c(23, 26, 45, 22, 43, 91, 82, 12, 57, 2)\n\n\nSpecial Vectors\nYou can create a sequence of numbers without needing to write them all down. Just list the first and last number of the sequence in your code\n\nSequence &lt;- seq(1,10)\n\nYou can also repeat numbers\n\nRepeat &lt;- rep(10, times = 10)\n\nAnd of course you can always store it in an object\n\nRepeat &lt;- rep(10, times = 25)\nRepeat\n\n [1] 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n\n\nThen you can use that to make a new database\nFirst we’ll add a sequence of numbers to match our repeat variable\n\nSequence &lt;- seq(1,25)\nSequence\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n\n\nThen create a dataset using the data.frame command and include the two objects that were just recently created.\n\nDataset &lt;- data.frame(Repeat, Sequence)\nDataset\n\n   Repeat Sequence\n1      10        1\n2      10        2\n3      10        3\n4      10        4\n5      10        5\n6      10        6\n7      10        7\n8      10        8\n9      10        9\n10     10       10\n11     10       11\n12     10       12\n13     10       13\n14     10       14\n15     10       15\n16     10       16\n17     10       17\n18     10       18\n19     10       19\n20     10       20\n21     10       21\n22     10       22\n23     10       23\n24     10       24\n25     10       25",
    "crumbs": [
      "R Basics",
      "Intro to R & R Studio"
    ]
  },
  {
    "objectID": "ANCOVA.html",
    "href": "ANCOVA.html",
    "title": "ANCOVA",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(car)\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some",
    "crumbs": [
      "Statistical Tests",
      "ANCOVA"
    ]
  },
  {
    "objectID": "ANCOVA.html#graphing-the-results",
    "href": "ANCOVA.html#graphing-the-results",
    "title": "ANCOVA",
    "section": "Graphing the results",
    "text": "Graphing the results\nFor graphing the results, a bar graph is the best way to look at the means. We can use the same basic formula we used for the one way anova to find our descriptive statistics.\n\nANCOVA_Descriptives &lt;- Puppy_love |&gt;\n  group_by(Dose) |&gt;\n  summarize(n = n(),\n            mean = mean(Happiness),\n            sd = sd(Happiness),\n            se = sd / sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\n\nAnd then create the graph\n\nggplot(ANCOVA_Descriptives, \n       aes(x = Dose, \n           y = mean)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin=mean-ci,\n                    ymax=mean+ci))\n\n\n\n\n\n\n\n\nOf course, it’s better to improve the graph a bit.\n\nggplot(ANCOVA_Descriptives, \n       aes(x = Dose,\n           y = mean)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\", fill=\"steelblue\") +\n  geom_errorbar(aes(ymin=mean-ci,\n                    ymax=mean+ci), width=.3, size=1) +\n  labs(title = \"Does Puppy Therapy Effect Happiness?\", \n       y=\"Mean Level of Happiness\", x=\"Time Spent in Puppy Therapy\") \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.",
    "crumbs": [
      "Statistical Tests",
      "ANCOVA"
    ]
  },
  {
    "objectID": "Chi Square.html",
    "href": "Chi Square.html",
    "title": "Chi Square",
    "section": "",
    "text": "Introduction to Chi-Square\nNot all data that is collected meets the standards for being parametric or based on populations. Some data that is collected is based on frequencies. Polling data is often of this type. If a person is interested in who might win the next election, they may run a poll like many did in the most recent presidential election between Biden and Trump.\nWhat makes data based on frequencies different is that the mean cannot be used for analysis. Frequency data is often based on categories or nominal data, thus it doesn’t make sense to compare these variables based on the types of statistical analyses we’ve done so far. Instead, we’ll use what is known as a Chi-Square test. \\[\nChi\\;Square\\;Test = \\chi^2\n\\] Let’s start with a very simple example. Perhaps, you are trying to decide whether a certain proposition will pass in the county and you want to decide if there is a preference. You collect data from 200 participants about whether they are in favor of the proposition or against the proposition on a simple yes vs. no question.\nIf there is no preference in the county, what would you expect the outcome of your survey to be? Well, if about the same amount of people were for the proposition as were against it, the outcome would be 50% yes and 50% no or half the participants would be against it, half for it. This is called the expected frequency or the expected frequency if the null hypothesis is correct, which in this case would be no particular preference in the sample for the proposition. Here’s the formula. \\[\nf_e = \\frac {total\\;in\\;survey}{number\\;of\\;categories}= \\frac {200}{2}\n\\] So in this case the expected frequency would be 100. So if there was no preference for a particular county proposition we would expect 100 persons to be for it and 100 persons to be against it. So Let’s say here is what the actual data looked like. 150 were for it and 50 were against it. This is called the observed frequency. \\[\nobserved\\;frequency = f_o\n\\] The chi square test is a combination of these two numbers. Basically the larger the difference between the observed and expected frequencies, the more likely there is a real preference either for or against the proposition. Here is the formula. \\[\nX^2 = \\Sigma \\frac{(f_o-f_e)^2}{f_e}\n\\] So the chi square test is the sum of the observed minus expected frequencies squared divided by the expected frequencies for each cell. In this case we have 2 cells one for those who answered yes in favor of the propsosition (150) and those who answered no against the proposition (50). So the computation would look like this: \\[\n\\frac{(150-100)^2}{100}+\\frac{(50-100)^2}{100}\n\\]\nSo in this case our answer turns out to be a Chi Square value of 50, which definitely reaches statistical significance (p &lt; .001). Thus there does seem to be a preference in favor for the proposition in the county.\nR allows for the same types of statistical analysis without having to calculate the entire formula.\nThe chi square test just calculated is called a single variable chi square because we are just looking at a single variable. It can also be calculated using R.\nFirst we need to create a contingency table, which is simply a table of contingent frequencies based on the data we’ve collected. Here I’ll use a tibble to create the dataset.\n\nProposition_Data &lt;- tribble(\n                  ~Yes, ~No,\n                  150, 50)\n\nThen we just simply run the chi square test. Note the code correct = FALSE. We add this because we don’t want to use the continuity correction, which is necessary when expected frequencies are below 5 (Remember in this instance our expected frequency would be 100).\n\nchisq.test(Proposition_Data, correct = FALSE)\n\n\n    Chi-squared test for given probabilities\n\ndata:  Proposition_Data\nX-squared = 50, df = 1, p-value = 1.537e-12\n\n\n\n\nTwo Variable Chi-Square\nAnother way that the chi square test is used is the testing of relationships between variables. Thus, are the variables related to each other or are they independent of each other?\nFor example, Look at dataset ch19ds2.\n\nhead(ch19ds2)\n\n   Sex Vote\n1 Male  Yes\n2 Male  Yes\n3 Male  Yes\n4 Male  Yes\n5 Male  Yes\n6 Male  Yes\n\n\nThere are two variables, gender (labeled here sex) and Vote, which was whether they voted yes or no on a recent ballot measure. First let’s look at a contingency table to get an overview of the data.\n\nVote_Gender_table &lt;- table(ch19ds2)\nVote_Gender_table\n\n        Vote\nSex      No Yes\n  Female 31  32\n  Male   20  37\n\n\nSo we want to see whether there was a relationship between gender and how persons voted on a particular measure. Here again, the Null hypothesis would assume that these variables are independent of each other. The frequency of No and Yes would be roughly proportional for both males and females. The alternative hypothesis assumes these frequencies are different based on whether you are a male or female.\nTo run a chi-square test and construct a graph we need to create a different way of representing the data called a matrix. A matrix is a display of different variables using dimensions. A matrix counts how many cases there are for various categories.\nFor example, based on our Chi_table above there are 31 females who voted No, 20 males who votes no, 32 females who voted yes and 37 males who voted yes. In order to run the Chi Square test, the data must be entered as a matrix.\nHere is how we create a matrix\n\n# Create a vector with frequency values\n# Write the values by filling in the rows of the first\n# column, then the second, and so on. \nMatrix1 &lt;- c(31, 20, 32, 37)\n\n#Change the vector into a matrix by assigning the dimensions. \n#In this case we want a 2 x 2 matrix, so 2 rows and 2 columns\ndim(Matrix1) &lt;- c(2, 2)\n\n# change column names\ncolnames(Matrix1) &lt;- c(\"No\",\"Yes\")\n# change row names\nrownames(Matrix1) &lt;- c(\"Female\",\"Male\")\n\n#Check out the finished product\nMatrix1\n\n       No Yes\nFemale 31  32\nMale   20  37\n\n\nLook a little bit closer at the code.\n\nStart with creating an object as a vector of numbers, in this case we called it Matrix1. The numbers will fill in starting at row 1 column 1 and fill in column 1 followed by column 2.\nNext, turn the vector into a matrix using the dim() argument. Use the “cbind” argument c(), to specify how many rows and columns you need. In this case we want 2 rows and 2 columns. The first number is the number of rows and the second number is the number of columns.\nFinally, use the colnames and rownames arguments to name what you want your rows and columns to be.\nThen you’re ready for the Chi Square test! \\(X^2\\)\n\nPerforming the test is fairly straight forward. The matrix is the dataset and the continuity correction is unnecessary because our expected frequencies for each cell is above 5. So we add the code, correct = FALSE, after the name of the matrix.\n\nchisq.test(Matrix1, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  Matrix1\nX-squared = 2.441, df = 1, p-value = 0.1182\n\n\nThe chi square value is very low and the p value is well above .05, so there is not a relationship between these two variables.\n\n\nEffect size for chi square\nFor an effect size, we use the odds ratio, which looks at the odds of the outcome we obtained. Obviously the higher the odds ratio the stronger the relationship. To find the odds ratio, first let’s look at the ratio for yes vs. no based on gender. For no it was 31 females to 20 males.\n\n31/20\n\n[1] 1.55\n\n\nSo this was 1.55 and we’ll also look at yes.\n\n32/37\n\n[1] 0.8648649\n\n\nSo the odds of their being a difference between males and females on yes vs. no. would be dividing these two numbers.\n\n(31/20)/(32/37)\n\n[1] 1.792188\n\n\nSo females were 1.79 times more likely to answer no then yes on the survey, which isn’t very large and since the overall test was not significant the effect size is not evaluated.\nNext, a bar graph helps to show the differences in frequencies and the direction of the differences. In this case, the base R package can be used to create a simple bar graph based on the frequencies or counts in the cells using the barplot argument.\n\nbarplot(Matrix1, beside = TRUE,\n        col = c(\"red\", \"green\"), legend.text = TRUE, \n        xlab = \"Voting based on Gender\", \n        ylab = \"Frequency of Yes and No Votes\", \n        main = \"Gender and Voting Frequency on Recent Bill\")\n\n\n\n\n\n\n\n\nFinally, here is how you report the results:\nThere was a not a significant association between gender and their voting preference (yes vs. no), \\(\\chi^2(1) = 2.441; p = 0.12\\). Based on the odds ratio, females were 1.8 times more likely to vote no than males.",
    "crumbs": [
      "Statistical Tests",
      "Chi Square"
    ]
  },
  {
    "objectID": "Two-Way ANOVA.html",
    "href": "Two-Way ANOVA.html",
    "title": "Two-Way ANOVA",
    "section": "",
    "text": "Before starting this section please review the section One-Way ANOVA first\n\n\nOne-way ANOVA is based on the general idea that the total variability \\(SS_T\\) is partitioned (divided or separated) into two types of variability. The variability between the groups \\(SS_{between}\\) and the variability within the groups \\(SS_{within}\\). Remember that the variability between the groups needed to be greater than the variability within the groups because that would indicate that the difference between the groups was greater than the measurement error that existed within the groups.\n\n\nTwo-way ANOVA goes beyond one-way ANOVA by analyzing the effects of two independent variables in the same experiment. In a two-way ANOVA the two independent variables are called main effects or factors and each main effect has its own individual hypothesis.\n\n\n\nFinally, the two-way ANOVA analyzes if there is an interaction effect between the two main effects or factors. An interaction effect occurs when the effect of one of the independent variables is not the same at all levels of the second independent variable. So the set up for a two-way ANOVA is:\n\nIndependent variable 1 = Main effect 1 = Factor 1\nIndependent variable 2 = Main effect 2 = Factor 2\nInteraction effect = Interaction between Factors 1 & 2\n\n\n\n\nThe example dataset tests what’s called the “beer googles effect”. Sometimes alcohol can have an effect on perceptions of attraction for potential dates, especially later in the evening at bars. The dataset tests whether perceptions of attractiveness change after drinking alcohol and whether males and females are effected by this phenomenon differently.\n\n\nTwo Independent Variables (Main Effects or Factors)\n\nMain Effect 1 = Alcohol 3 Levels (None, 2 Pints, 4 Pints)\nMain Effect 2 = Gender 2 Levels (Male and Female)\nInteraction Effect = Gender x Alcohol\nDV = Attractiveness of the partner selected at the end of the evening\n\nAlternative Hypotheses\n\n\\(H_1\\) Alcohol has an effect on the attractiveness level of the selected partner\n\\(H_2\\) Gender has an effect on the attractiveness level of the partner.\n\\(H_3\\) There is an interaction effect between Alcohol and Gender\n\nSum of Squares variation estimates\n\n\n\n\n\nGet the dataset and import it\n\nlibrary(haven)\ngoggles &lt;- read_sav(\"goggles.sav\")\nView(goggles)\n\nCheck out Gender variable\n\ngoggles$Gender\n\n&lt;labelled&lt;double&gt;[48]&gt;: Gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0\n\nLabels:\n value  label\n     0   Male\n     1 Female\n\n\nCheck out Alcohol variable\n\ngoggles$Alcohol\n\n&lt;labelled&lt;double&gt;[48]&gt;: Alcohol Consumption\n [1] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n[39] 2 2 3 3 3 3 3 3 3 3\n\nLabels:\n value   label\n     1    None\n     2 2 Pints\n     3 4 Pints\n\n\nCheck out Attractiveness variable\n\ngoggles$Attractiveness\n\n [1] 65 70 60 60 60 55 60 55 70 65 60 70 65 60 60 50 55 65 70 55 55 60 50 50 50\n[26] 55 80 65 70 75 75 65 45 60 85 65 70 70 80 60 30 30 30 55 35 20 45 40\nattr(,\"label\")\n[1] \"Attractiveness of Date\"\nattr(,\"format.spss\")\n[1] \"F8.0\"\nattr(,\"display_width\")\n[1] 13\n\n\nAlcohol and Gender variables are factors. When they get imported from SPSS they don’t function as well because the focus is on the numbers, not the labels or words.We can use tidyverse and the mutate function to fix this.\n\ngoggles &lt;- goggles %&gt;% \n  mutate(Gender = factor(Gender, levels = c(0,1), \n         labels = c(\"Male\", \"Female\")))\n\nWe can do the same thing with the Alcohol variable. Make sure you know the levels or numbering of the variable\n\ngoggles$Alcohol\n\n&lt;labelled&lt;double&gt;[48]&gt;: Alcohol Consumption\n [1] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n[39] 2 2 3 3 3 3 3 3 3 3\n\nLabels:\n value   label\n     1    None\n     2 2 Pints\n     3 4 Pints\n\n\nThen go ahead and mutate the variable as you did with Gender\n\ngoggles &lt;- goggles %&gt;% \n  mutate(Alcohol = factor(Alcohol, levels = c(1,2,3), \n            labels = c(\"None\", \"2 Pints\", \"4 Pints\")))\n\n\n\nNow we can move to the Two-Way ANOVA analysis. “m3” is the new object to save results so the formula will have this structure.\n\n# m3 &lt;- aov (this is the computation you are using, like t.test) Then the rest of your formula should look like this (Dependent variable ~ Varible 1 + Variable 2 + Variable 1*Variable 2, data = [your dataset])\n\n\nm3 &lt;- aov(Attractiveness ~ Gender + Alcohol + \n           Gender*Alcohol, data = goggles)\n\nCheck the results\n\nsummary(m3)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGender          1    169   168.8   2.032    0.161    \nAlcohol         2   3332  1666.1  20.065 7.65e-07 ***\nGender:Alcohol  2   1978   989.1  11.911 7.99e-05 ***\nResiduals      42   3487    83.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nKey Findings from our output\nMain effect of Alcohol on attractiveness\nInteraction effect of Alcohol and Gender\nWhat does this mean?\n\n\n\n\nLet’s use a graph to understand this better\nFirst let’s look at each variable individually\n\nGogglesAlcohol &lt;- goggles %&gt;%\n  group_by(Alcohol) %&gt;%\n  summarize(n = n(),\n            mean = mean(Attractiveness),\n            sd = sd(Attractiveness),\n            se = sd/sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\n\nGraph the Alcohol variable individually\n\nggplot(GogglesAlcohol, aes(x = Alcohol,\n                                y = mean)) +\n  geom_point(size = 3) +\n  geom_line(size = 1) +\n  geom_errorbar(aes(ymin  =mean - ci, \n                    ymax = mean + ci), \n                width = .1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nGraph the Gender variable individually\n\nGogglesGender &lt;- goggles %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(n = n(),\n            mean = mean(Attractiveness),\n            sd = sd(Attractiveness),\n            se = sd/sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\n\nGraph it\n\nggplot(GogglesGender, aes(x = Gender,\n                           y = mean)) +\n  geom_point(size = 3) +\n  geom_line(size = 1) +\n  geom_errorbar(aes(ymin  =mean - ci, \n                    ymax = mean + ci), \n                width = .1) +\n  ylim(0,70)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nGraph the Interaction Effect\nFinally, we can graph relationships for both variables.\nFirst find your descriptive statistics, but this time based on two independent variables\n\nGogglesDescriptives &lt;- goggles %&gt;%\n  group_by(Alcohol, Gender) %&gt;%\n  summarize(n = n(),\n            mean = mean(Attractiveness),\n            sd = sd(Attractiveness),\n            se = sd/sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\n\n`summarise()` has grouped output by 'Alcohol'. You can override using the\n`.groups` argument.\n\n\nCheck it\n\nGogglesDescriptives\n\n# A tibble: 6 × 7\n# Groups:   Alcohol [3]\n  Alcohol Gender     n  mean    sd    se    ci\n  &lt;fct&gt;   &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 None    Male       8  66.9 10.3   3.65  8.64\n2 None    Female     8  60.6  4.96  1.75  4.14\n3 2 Pints Male       8  66.9 12.5   4.43 10.5 \n4 2 Pints Female     8  62.5  6.55  2.31  5.47\n5 4 Pints Male       8  35.6 10.8   3.83  9.06\n6 4 Pints Female     8  57.5  7.07  2.5   5.91\n\n\nUse a line graph to graph the relationship\n\nggplot(GogglesDescriptives, aes(x = Alcohol,\n                     y = mean, \n                     group=Gender, \n                     color=Gender)) +\n  geom_point(size = 3) +\n  geom_line(size = 1) +\n  geom_errorbar(aes(ymin  =mean - ci, \n                    ymax = mean + ci), \n                width = .1)\n\n\n\n\n\n\n\n\nUse dodge functions to make graph clearer\n\npd &lt;- position_dodge(0.2)\nggplot(GogglesDescriptives, \n       aes(x = Alcohol, \n           y = mean, \n           group=Gender, \n           color=Gender)) +\n  geom_point(position = pd, \n             size = 3) +\n  geom_line(position = pd,\n            size = 1) +\n  geom_errorbar(aes(ymin = mean - ci, \n                    ymax = mean + ci), \n                width = .1, \n                position= pd)\n\n\n\n\n\n\n\n\nAll the bells and whistles\n\npd &lt;- position_dodge(0.2)\nggplot(GogglesDescriptives, \n       aes(x = Alcohol, \n           y = mean, \n           group=Gender, \n           color=Gender)) +\n  geom_point(position=pd, \n             size = 3) +\n  geom_line(position = pd, \n            size = 1) +\n  geom_errorbar(aes(ymin = mean - ci, \n                    ymax = mean + ci), \n                width = .1, \n                position = pd, \n                size = 1) +\n  scale_color_brewer(palette=\"Set1\") +\n  theme_minimal() +\n  labs(title = \"Beer Goggels Effect\",\n       x = \"Amount of Alcohol Consumed\", \n       y = \"Mean Level of Attractiveness\",\n       color = \"Gender\")\n\n\n\n\n\n\n\n\nUse Tukey to look at specific differences in the groups you are interested in\n\nTukeyHSD(m3)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Attractiveness ~ Gender + Alcohol + Gender * Alcohol, data = goggles)\n\n$Gender\n            diff       lwr      upr     p adj\nFemale-Male 3.75 -1.558607 9.058607 0.1613818\n\n$Alcohol\n                    diff        lwr        upr     p adj\n2 Pints-None      0.9375  -6.889643   8.764643 0.9544456\n4 Pints-None    -17.1875 -25.014643  -9.360357 0.0000105\n4 Pints-2 Pints -18.1250 -25.952143 -10.297857 0.0000040\n\n$`Gender:Alcohol`\n                                 diff        lwr        upr     p adj\nFemale:None-Male:None          -6.250 -19.851381   7.351381 0.7432243\nMale:2 Pints-Male:None          0.000 -13.601381  13.601381 1.0000000\nFemale:2 Pints-Male:None       -4.375 -17.976381   9.226381 0.9277939\nMale:4 Pints-Male:None        -31.250 -44.851381 -17.648619 0.0000003\nFemale:4 Pints-Male:None       -9.375 -22.976381   4.226381 0.3286654\nMale:2 Pints-Female:None        6.250  -7.351381  19.851381 0.7432243\nFemale:2 Pints-Female:None      1.875 -11.726381  15.476381 0.9983764\nMale:4 Pints-Female:None      -25.000 -38.601381 -11.398619 0.0000306\nFemale:4 Pints-Female:None     -3.125 -16.726381  10.476381 0.9825753\nFemale:2 Pints-Male:2 Pints    -4.375 -17.976381   9.226381 0.9277939\nMale:4 Pints-Male:2 Pints     -31.250 -44.851381 -17.648619 0.0000003\nFemale:4 Pints-Male:2 Pints    -9.375 -22.976381   4.226381 0.3286654\nMale:4 Pints-Female:2 Pints   -26.875 -40.476381 -13.273619 0.0000080\nFemale:4 Pints-Female:2 Pints  -5.000 -18.601381   8.601381 0.8796489\nFemale:4 Pints-Male:4 Pints    21.875   8.273619  35.476381 0.0002776\n\n\nFor us, the most important difference is that males are more likely to choose a less attractive person at 4 pints of alcohol than females\n\n\n\n\n\nThere was a significant main effect of the amount of alcohol consumed on the attractiveness of the date that was selected, F(2, 42) = 20.07, p &lt; .001.\n\n\nThere was not a significant main effect of gender on the attractiveness of the date that was selected, F(1, 42) = 2.03, p = .161.\n\n\n\n\n\nThere was a significant interaction effect between amount of alcohol consumed and gender on the attractiveness of the date that was selected F(2, 42) = 11.91, p &lt; .001.\n\n\n\n\n\nTukeyHSD post hoc tests revealed that at the largest amount of alcohol consumption (4 pints) Males were significantly more likely to choose a less attractive date (M=35.6, SE=3.83) in comparison to females (M=57.5, SE=57.5). This difference, 21.88, 95% CI[8.27, 35.48] was significant with an adjusted p = .0003.",
    "crumbs": [
      "Statistical Tests",
      "Two-Way ANOVA"
    ]
  },
  {
    "objectID": "Two-Way ANOVA.html#two-way-anova",
    "href": "Two-Way ANOVA.html#two-way-anova",
    "title": "Two-Way ANOVA",
    "section": "",
    "text": "Before starting this section please review the section One-Way ANOVA first\n\n\nOne-way ANOVA is based on the general idea that the total variability \\(SS_T\\) is partitioned (divided or separated) into two types of variability. The variability between the groups \\(SS_{between}\\) and the variability within the groups \\(SS_{within}\\). Remember that the variability between the groups needed to be greater than the variability within the groups because that would indicate that the difference between the groups was greater than the measurement error that existed within the groups.\n\n\nTwo-way ANOVA goes beyond one-way ANOVA by analyzing the effects of two independent variables in the same experiment. In a two-way ANOVA the two independent variables are called main effects or factors and each main effect has its own individual hypothesis.\n\n\n\nFinally, the two-way ANOVA analyzes if there is an interaction effect between the two main effects or factors. An interaction effect occurs when the effect of one of the independent variables is not the same at all levels of the second independent variable. So the set up for a two-way ANOVA is:\n\nIndependent variable 1 = Main effect 1 = Factor 1\nIndependent variable 2 = Main effect 2 = Factor 2\nInteraction effect = Interaction between Factors 1 & 2\n\n\n\n\nThe example dataset tests what’s called the “beer googles effect”. Sometimes alcohol can have an effect on perceptions of attraction for potential dates, especially later in the evening at bars. The dataset tests whether perceptions of attractiveness change after drinking alcohol and whether males and females are effected by this phenomenon differently.\n\n\nTwo Independent Variables (Main Effects or Factors)\n\nMain Effect 1 = Alcohol 3 Levels (None, 2 Pints, 4 Pints)\nMain Effect 2 = Gender 2 Levels (Male and Female)\nInteraction Effect = Gender x Alcohol\nDV = Attractiveness of the partner selected at the end of the evening\n\nAlternative Hypotheses\n\n\\(H_1\\) Alcohol has an effect on the attractiveness level of the selected partner\n\\(H_2\\) Gender has an effect on the attractiveness level of the partner.\n\\(H_3\\) There is an interaction effect between Alcohol and Gender\n\nSum of Squares variation estimates\n\n\n\n\n\nGet the dataset and import it\n\nlibrary(haven)\ngoggles &lt;- read_sav(\"goggles.sav\")\nView(goggles)\n\nCheck out Gender variable\n\ngoggles$Gender\n\n&lt;labelled&lt;double&gt;[48]&gt;: Gender\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[39] 0 0 0 0 0 0 0 0 0 0\n\nLabels:\n value  label\n     0   Male\n     1 Female\n\n\nCheck out Alcohol variable\n\ngoggles$Alcohol\n\n&lt;labelled&lt;double&gt;[48]&gt;: Alcohol Consumption\n [1] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n[39] 2 2 3 3 3 3 3 3 3 3\n\nLabels:\n value   label\n     1    None\n     2 2 Pints\n     3 4 Pints\n\n\nCheck out Attractiveness variable\n\ngoggles$Attractiveness\n\n [1] 65 70 60 60 60 55 60 55 70 65 60 70 65 60 60 50 55 65 70 55 55 60 50 50 50\n[26] 55 80 65 70 75 75 65 45 60 85 65 70 70 80 60 30 30 30 55 35 20 45 40\nattr(,\"label\")\n[1] \"Attractiveness of Date\"\nattr(,\"format.spss\")\n[1] \"F8.0\"\nattr(,\"display_width\")\n[1] 13\n\n\nAlcohol and Gender variables are factors. When they get imported from SPSS they don’t function as well because the focus is on the numbers, not the labels or words.We can use tidyverse and the mutate function to fix this.\n\ngoggles &lt;- goggles %&gt;% \n  mutate(Gender = factor(Gender, levels = c(0,1), \n         labels = c(\"Male\", \"Female\")))\n\nWe can do the same thing with the Alcohol variable. Make sure you know the levels or numbering of the variable\n\ngoggles$Alcohol\n\n&lt;labelled&lt;double&gt;[48]&gt;: Alcohol Consumption\n [1] 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 2 2 2 2 2 2\n[39] 2 2 3 3 3 3 3 3 3 3\n\nLabels:\n value   label\n     1    None\n     2 2 Pints\n     3 4 Pints\n\n\nThen go ahead and mutate the variable as you did with Gender\n\ngoggles &lt;- goggles %&gt;% \n  mutate(Alcohol = factor(Alcohol, levels = c(1,2,3), \n            labels = c(\"None\", \"2 Pints\", \"4 Pints\")))\n\n\n\nNow we can move to the Two-Way ANOVA analysis. “m3” is the new object to save results so the formula will have this structure.\n\n# m3 &lt;- aov (this is the computation you are using, like t.test) Then the rest of your formula should look like this (Dependent variable ~ Varible 1 + Variable 2 + Variable 1*Variable 2, data = [your dataset])\n\n\nm3 &lt;- aov(Attractiveness ~ Gender + Alcohol + \n           Gender*Alcohol, data = goggles)\n\nCheck the results\n\nsummary(m3)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGender          1    169   168.8   2.032    0.161    \nAlcohol         2   3332  1666.1  20.065 7.65e-07 ***\nGender:Alcohol  2   1978   989.1  11.911 7.99e-05 ***\nResiduals      42   3487    83.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nKey Findings from our output\nMain effect of Alcohol on attractiveness\nInteraction effect of Alcohol and Gender\nWhat does this mean?\n\n\n\n\nLet’s use a graph to understand this better\nFirst let’s look at each variable individually\n\nGogglesAlcohol &lt;- goggles %&gt;%\n  group_by(Alcohol) %&gt;%\n  summarize(n = n(),\n            mean = mean(Attractiveness),\n            sd = sd(Attractiveness),\n            se = sd/sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\n\nGraph the Alcohol variable individually\n\nggplot(GogglesAlcohol, aes(x = Alcohol,\n                                y = mean)) +\n  geom_point(size = 3) +\n  geom_line(size = 1) +\n  geom_errorbar(aes(ymin  =mean - ci, \n                    ymax = mean + ci), \n                width = .1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nGraph the Gender variable individually\n\nGogglesGender &lt;- goggles %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(n = n(),\n            mean = mean(Attractiveness),\n            sd = sd(Attractiveness),\n            se = sd/sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\n\nGraph it\n\nggplot(GogglesGender, aes(x = Gender,\n                           y = mean)) +\n  geom_point(size = 3) +\n  geom_line(size = 1) +\n  geom_errorbar(aes(ymin  =mean - ci, \n                    ymax = mean + ci), \n                width = .1) +\n  ylim(0,70)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nGraph the Interaction Effect\nFinally, we can graph relationships for both variables.\nFirst find your descriptive statistics, but this time based on two independent variables\n\nGogglesDescriptives &lt;- goggles %&gt;%\n  group_by(Alcohol, Gender) %&gt;%\n  summarize(n = n(),\n            mean = mean(Attractiveness),\n            sd = sd(Attractiveness),\n            se = sd/sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\n\n`summarise()` has grouped output by 'Alcohol'. You can override using the\n`.groups` argument.\n\n\nCheck it\n\nGogglesDescriptives\n\n# A tibble: 6 × 7\n# Groups:   Alcohol [3]\n  Alcohol Gender     n  mean    sd    se    ci\n  &lt;fct&gt;   &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 None    Male       8  66.9 10.3   3.65  8.64\n2 None    Female     8  60.6  4.96  1.75  4.14\n3 2 Pints Male       8  66.9 12.5   4.43 10.5 \n4 2 Pints Female     8  62.5  6.55  2.31  5.47\n5 4 Pints Male       8  35.6 10.8   3.83  9.06\n6 4 Pints Female     8  57.5  7.07  2.5   5.91\n\n\nUse a line graph to graph the relationship\n\nggplot(GogglesDescriptives, aes(x = Alcohol,\n                     y = mean, \n                     group=Gender, \n                     color=Gender)) +\n  geom_point(size = 3) +\n  geom_line(size = 1) +\n  geom_errorbar(aes(ymin  =mean - ci, \n                    ymax = mean + ci), \n                width = .1)\n\n\n\n\n\n\n\n\nUse dodge functions to make graph clearer\n\npd &lt;- position_dodge(0.2)\nggplot(GogglesDescriptives, \n       aes(x = Alcohol, \n           y = mean, \n           group=Gender, \n           color=Gender)) +\n  geom_point(position = pd, \n             size = 3) +\n  geom_line(position = pd,\n            size = 1) +\n  geom_errorbar(aes(ymin = mean - ci, \n                    ymax = mean + ci), \n                width = .1, \n                position= pd)\n\n\n\n\n\n\n\n\nAll the bells and whistles\n\npd &lt;- position_dodge(0.2)\nggplot(GogglesDescriptives, \n       aes(x = Alcohol, \n           y = mean, \n           group=Gender, \n           color=Gender)) +\n  geom_point(position=pd, \n             size = 3) +\n  geom_line(position = pd, \n            size = 1) +\n  geom_errorbar(aes(ymin = mean - ci, \n                    ymax = mean + ci), \n                width = .1, \n                position = pd, \n                size = 1) +\n  scale_color_brewer(palette=\"Set1\") +\n  theme_minimal() +\n  labs(title = \"Beer Goggels Effect\",\n       x = \"Amount of Alcohol Consumed\", \n       y = \"Mean Level of Attractiveness\",\n       color = \"Gender\")\n\n\n\n\n\n\n\n\nUse Tukey to look at specific differences in the groups you are interested in\n\nTukeyHSD(m3)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Attractiveness ~ Gender + Alcohol + Gender * Alcohol, data = goggles)\n\n$Gender\n            diff       lwr      upr     p adj\nFemale-Male 3.75 -1.558607 9.058607 0.1613818\n\n$Alcohol\n                    diff        lwr        upr     p adj\n2 Pints-None      0.9375  -6.889643   8.764643 0.9544456\n4 Pints-None    -17.1875 -25.014643  -9.360357 0.0000105\n4 Pints-2 Pints -18.1250 -25.952143 -10.297857 0.0000040\n\n$`Gender:Alcohol`\n                                 diff        lwr        upr     p adj\nFemale:None-Male:None          -6.250 -19.851381   7.351381 0.7432243\nMale:2 Pints-Male:None          0.000 -13.601381  13.601381 1.0000000\nFemale:2 Pints-Male:None       -4.375 -17.976381   9.226381 0.9277939\nMale:4 Pints-Male:None        -31.250 -44.851381 -17.648619 0.0000003\nFemale:4 Pints-Male:None       -9.375 -22.976381   4.226381 0.3286654\nMale:2 Pints-Female:None        6.250  -7.351381  19.851381 0.7432243\nFemale:2 Pints-Female:None      1.875 -11.726381  15.476381 0.9983764\nMale:4 Pints-Female:None      -25.000 -38.601381 -11.398619 0.0000306\nFemale:4 Pints-Female:None     -3.125 -16.726381  10.476381 0.9825753\nFemale:2 Pints-Male:2 Pints    -4.375 -17.976381   9.226381 0.9277939\nMale:4 Pints-Male:2 Pints     -31.250 -44.851381 -17.648619 0.0000003\nFemale:4 Pints-Male:2 Pints    -9.375 -22.976381   4.226381 0.3286654\nMale:4 Pints-Female:2 Pints   -26.875 -40.476381 -13.273619 0.0000080\nFemale:4 Pints-Female:2 Pints  -5.000 -18.601381   8.601381 0.8796489\nFemale:4 Pints-Male:4 Pints    21.875   8.273619  35.476381 0.0002776\n\n\nFor us, the most important difference is that males are more likely to choose a less attractive person at 4 pints of alcohol than females\n\n\n\n\n\nThere was a significant main effect of the amount of alcohol consumed on the attractiveness of the date that was selected, F(2, 42) = 20.07, p &lt; .001.\n\n\nThere was not a significant main effect of gender on the attractiveness of the date that was selected, F(1, 42) = 2.03, p = .161.\n\n\n\n\n\nThere was a significant interaction effect between amount of alcohol consumed and gender on the attractiveness of the date that was selected F(2, 42) = 11.91, p &lt; .001.\n\n\n\n\n\nTukeyHSD post hoc tests revealed that at the largest amount of alcohol consumption (4 pints) Males were significantly more likely to choose a less attractive date (M=35.6, SE=3.83) in comparison to females (M=57.5, SE=57.5). This difference, 21.88, 95% CI[8.27, 35.48] was significant with an adjusted p = .0003.",
    "crumbs": [
      "Statistical Tests",
      "Two-Way ANOVA"
    ]
  },
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "Regression",
    "section": "",
    "text": "Regression\nUnderstanding Linear regression.\nAs we saw with correlation, one way we can understand a relationship is through using a line as a model. Lines require two points and when using a scatterplot, based on the slope, an estimate of the strength of the relationship between two variables can be assessed. Linear regression adds more tools to understand the relationship between two variables.\nLet’s start with the basic equation for a line. \\[\nY=b_0 + b_1X\n\\] \\(b_1\\) tells us the shape of the model, basically whether it’s positive or negative, just like we saw with correlational coefficients earlier. \\(b_0\\) stands for our Y intercept or where the line crosses the Y axis. So these two values essentially stand in for our two points on a line. One indicates the point on the Y axis where the line crosses the Y axis and the other one describes the slope of the line (either positive or negative) and the degree of the slope of the line.\nOne thing that regression lines enable is the prediction of one variable (the outcome variable or dependent variables) based on another (predictor variable or independent variable). This is a form of simple regression. If you are using more than one variable or several predictors to predict an outcome variable, this is known as multiple regression.\nFor example, going back to our Album Sales dataset. If we wanted to predict album sales (Y)(outcome variable) based on our advertising budget (X)(predictor variable) and the regression model had 50 as the constant and 100 as the gradient, the formula would look like this: \\[\nalbum\\;sales = b_0 + b_1(advertising\\;budget)\n\\] The we could supply the constant and the gradient \\[\nalbum\\;sales = 50 + (100 \\times advertising\\;budget)\n\\] Then we could solve for the number of albums we could sell if we spent 5 dollars on the advertising budget \\[\nalbum\\;sales = 50 + (100 \\times 5)\n\\] Thus in R\n\n50+(100*5)\n\n[1] 550\n\n\nThe values of 50 and 100 were just made up numbers. How do we find the actual intercepts and gradients using R?\nLet’s take a look at the code for running a linear regression in R\n\nregression &lt;- lm(formula = Sales ~ Adverts, data = Album_Sales)\n\nFor regression models, the output needs to be saved in an object, so there’s a second step to display the results.\n\nregression\n\n\nCall:\nlm(formula = Sales ~ Adverts, data = Album_Sales)\n\nCoefficients:\n(Intercept)      Adverts  \n  134.13994      0.09612  \n\n\nNotice that when the object is called up it provides the intercept and gradient discussed earlier. Thus we can plug these numbers into our formula using R and find the predicted album sales based on spending 500 dollars on the advertising budget.\n\n134.14 + (.096*500)\n\n[1] 182.14\n\n\nThe two coefficents have been identified, but we still don’t know if advertising is a good predictor of album sales. However, this information is also in the results that were saved in the object “regression”. To look at these results, the summary command is used.\n\nsummary(regression)\n\n\nCall:\nlm(formula = Sales ~ Adverts, data = Album_Sales)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-152.949  -43.796   -0.393   37.040  211.866 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***\nAdverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 65.99 on 198 degrees of freedom\nMultiple R-squared:  0.3346,    Adjusted R-squared:  0.3313 \nF-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nRemember that the variable of interest, the predictor variable, was advertising. If we worked at this record company we would be interested if money spent on advertising would help in increasing record sales. The first thing to notice is the F statistic, which analyzes whether the regression model is significant or not. Notice that this number is 99.59 with a p value well below .05, so that tells us our regression model is a good one.\nWhere did this statistics analysis come from though? To answer this question we need to look at little bit closer at how to analyze the fit of a line.\nFrom the discussion of correlation, you may remember that a regression line is a line that is used to fit the data as well as possible, but as with all statistics there is a certain amount of error. The regression line does not cross all the points on a scatterdot, but tries to be as close to as many points as possible. The difference between the line and a particular point is known as a residual. A residual is similar to a deviation, but it is the distance or error between the line and a particular point. The smaller the residual the better the line is a predictor for that particular point or score. The larger the residual the less predictive the regression line is for that score.\nTo evaluate the regression model, the average amount of residual error must be calculated. However, the same problem observed with trying to find the average deviation for a dataset remains here. If you add up all the residuals and try to divide by N, the residuals will add up to zero, which doesn’t help very much. We solve this problem the same way it was solved with deviations, we square them!. This number becomes the sum of squared deviations and is symbolized below: \\[\nSS_R\n\\] To decide whether the regression line is a good predictor, something must be used as a comparator. In this case, the mean is compared to the line to see which serves as a better predictor for the data. If the line has a low amount of error (low \\(SS_R\\)) and shows considerable improvement in comparison to the mean (high \\(SS_M\\)), the regression line is a good predictor. Sum of squares model (\\(SS_M\\)) is a measurement of the average distance between the line and the mean. This provides a measurement of the slope or gradient of the line. Remember that the steeper the slope or gradient of the line the stronger the relationship between two variables. Here the mean, (which is a horizontal line drawn on the Y axis) acts as an estimate of zero relationship or correlation. Thus, the greater the difference between the line and the mean, the greater the relationship or correlation between the variables and the better the predictor variable is at predicting the relationship.\nThere are a few more steps to get to the F value. 1. Each sum of squares must be divided by the degrees of freedom. Think back to finding the standard deviation. We divided the sum of squares by N -1. Sum of squares residual and sum of squares model each have a slightly different degrees of freedom. - k - 1 - N - k 2. Once we’ve divided by the degrees of freedom, we get two numbers, The Means squared model and the Means squared residual. This his how we get to the F value. \\[\nF = \\frac {MS_M}{MS_R}\n\\] F is a proportional value, so it starts at 1 and is never a negative value. The higher the F value the higher the amount of improvement demonstrated by the regression model in comparison to the residuals (error in the model).\nOf course, we also want to include a scatterplot to show the relationship.\n\nggplot(Album_Sales, mapping = aes(x = Adverts, y = Sales)) +\n         geom_point() +\n        geom_smooth(method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally, the results section should be properly formatted. Here is a good example:\n\nThe regression model does appear to be significant. Advertising budget is a good predictor of sales based on the ANOVA test, F (1/98)= 99.59, p &lt; .001. Based on the \\(R^2\\) value of .33, 33% of the sales variable can be explained by the advertising budget.",
    "crumbs": [
      "Statistical Tests",
      "Regression"
    ]
  },
  {
    "objectID": "Independent t test.html",
    "href": "Independent t test.html",
    "title": "Independent t-test",
    "section": "",
    "text": "Resources to consult:\n\nChapter 10 from Field, A. (2017). Discovering Statistics Using IBM SPSS Statistics (5th Edition). SAGE Publications, Ltd. (UK).\nSection from Learning Statistics with R",
    "crumbs": [
      "Intro to Statistics",
      "Independent t-test"
    ]
  },
  {
    "objectID": "Independent t test.html#resources",
    "href": "Independent t test.html#resources",
    "title": "Independent t-test",
    "section": "",
    "text": "Resources to consult:\n\nChapter 10 from Field, A. (2017). Discovering Statistics Using IBM SPSS Statistics (5th Edition). SAGE Publications, Ltd. (UK).\nSection from Learning Statistics with R",
    "crumbs": [
      "Intro to Statistics",
      "Independent t-test"
    ]
  },
  {
    "objectID": "Independent t test.html#review",
    "href": "Independent t test.html#review",
    "title": "Independent t-test",
    "section": "Review",
    "text": "Review\nIn the section on probability we learned about the binomial distribution, which is a type of probability distribution. Probability distributions allow us to look at the probability of different types of outcomes. For example, the binomial distribution enables the calculation of the probability associated with getting say 6 heads out of 25 flips of a fair-sided coin.\n\ndbinom(x = 6, size = 25, prob = 1/2)\n\n[1] 0.005277991\n\n\nFor the t-test we use the t distribution to analyze the probability of the outcome of a particular experiment. More specifically, an experiment with two groups. The t distribution is similar to the binomial distribution, but much closer to the z distribution that we learned about in the section on the normal curve and z scores.",
    "crumbs": [
      "Intro to Statistics",
      "Independent t-test"
    ]
  },
  {
    "objectID": "Independent t test.html#the-t-distribution",
    "href": "Independent t test.html#the-t-distribution",
    "title": "Independent t-test",
    "section": "The t Distribution",
    "text": "The t Distribution\nPagano (2013) provides a good definition of the t distribution.\n\nThe sampling distribution of t is a probability distribution of the t values that would occur if all possible different samples of a fixed size N were drawn from the null-hypothesis population. It gives (1) all the possible different t values for samples of size N and (2) the probability of getting each value if sampling is random from the null-hypothesis population p. 329\n\nLet’s try to unpack this a bit. First off, it’s a probability distribution constructed of t values or t scores based on all possible samples drawn from a population. Remember that samples are a smaller group taken from a population. The “fixed size N” refers to the size of the sample that was taken in the experiment, so the distribution will change it’s shape slightly based on N.\nAlthough the t distribution is based on the sample size, it’s actually constructed by using a somewhat complicated mathematical concept called degrees of freedom or df. Degrees of freedom refers to how many scores are free to vary for a given statistic. Rather, than go too far into the weeds with this concept, for now just know that the degrees of freedom for the independent t test is \\(N - 2\\).\nSo if we had a sample size of 30, we would use a t distribution of \\(df=30 - 2\\) or \\(df=28\\) to construct the distribution.\n\n\n\n\n\n\n\n\n\nThe t distribution changes shape based on the degrees of freedom (df) that is associated with it, which is related to the sample size.\n\n\n\n\n\n\n\n\n\nNotice that the t distribution has the same shape as a normal curve, plus it looks like the z distribution with a mean of zero and positive values above the mean and negative values below the mean. Just like with the z distribution as the absolute value of the t score increases (remember that absolute value looks at the score and ignores the +/- sign), it is associated with a lower probability. Each t score has a particular probability associated with it.",
    "crumbs": [
      "Intro to Statistics",
      "Independent t-test"
    ]
  },
  {
    "objectID": "Independent t test.html#using-the-independent-t-test",
    "href": "Independent t test.html#using-the-independent-t-test",
    "title": "Independent t-test",
    "section": "Using the Independent t test",
    "text": "Using the Independent t test\nThe independent t test is used when analyzing the difference in means between two separate groups. Typically, this is used to analyze the difference between the control group (group who did not receive the independent variable) and the experimental group (group who did receive the independent variable).\n\nNull Hypothesis - Experimental Group = Control Group\n\n\nAlternative Hypothesis - Experimental Group \\(\\neq\\) Control Group\n\nThe null hypothesis assumes that the mean difference between the two groups is equal to zero. If the samples came from the same population, their means would be roughly the same and if they came from the same population their characteristics would be the same as well. Thus, the null hypothesis assumes no real differences based on the presence of the independent variable, while the alternative hypothesis assumes that the two means are different. \\[\nNull = \\bar X_{1} = \\bar X_{2}\n\\]\n\\[\nAlternative = \\bar X_{1} \\neq \\bar X_{2}\n\\]\nThis type of design is sometimes referred as a between-groups or independent design. It requires a categorical (nominal) predictor or independent variable that will specify the two groups (experimental and control) and a continuous dependent variable that specifies the outcome.\nThere are two possible reasons why two samples have different means.\n\nThe two sample means come from the same population, but the existence of measurement error and variability are the reason they are different. (Null hypothesis)\nThe two sample means come from different populations, which have different characteristics, and reflect a genuine difference between the sample means. (Alternative hypothesis)\n\n\nIndependent t test formula\nRemember the basic formula used in most statistical situations\n\\[\nStatistic = \\frac{Signal}{Noise}\n\\]\nThe signal refers to systematic variation or variation caused by the experimental manipulation and the work of the independent variable. If the independent variable has an effect we’ll expect there to be a difference between the means. Noise refers to background noise or unsystematic variation. Variation we don’t manipulate or don’t have control over, like measurement error.\nThus, the formula for the t test can be represented as:\n\\[\nt = \\frac{difference\\;between\\; sample\\;means}{\\;measurement\\; error}\n\\]\nBasically, the difference between the means is what we would expect if the independent variable has an effect. There should be some difference in the dependent variable that is a measure the effect of the independent variable. This is then compared to the measurement error. Remember that all statistical calculation contains some amount of error, but the difference between the means has to be greater that than measurement error in order to be considered statistically significant. So the numerator in the formula is simply the difference between the means of the two groups or \\(\\bar X_{1}-\\bar X _{2}\\).\nAs an estimate of the measurement error, the t test uses the standard deviation from the sample divided by the square root of n to calculate the measurement error. This is a form of the standard deviation, which was shown earlier as a descriptive statistic of measurement error. This statistic is known as the estimate of the standard error. So here is what our formula looks like now.\n\\[\nt = \\frac{\\bar X_{1}-\\bar X _{2}}{estimate\\; of\\; the\\; \\;standard\\; error}\n\\]\nHowever we need estimates from both sample 1 and sample 2 so the denominator of the equation looks like this: \\[\n\\sqrt{\\frac{s^{2}_{1}}{n_{1}}+\\frac{s^{2}_{2}}{n_{2}} }\n\\]\nFinally, we can put everything together for the primary formula. \\[\nt = \\frac{\\bar X_{1}- \\bar X_{2}}{\\sqrt{\\frac{s^{2}_{1}}{n_{1}}+\\frac{s^{2}_{2}}{n_{2}} }}\n\\]\nOverall, for the independent t-test, if the mean difference between the two samples is significantly greater than the measurement error in the two samples (which is found by using the estimated standard error), there is a real difference between the two samples.",
    "crumbs": [
      "Intro to Statistics",
      "Independent t-test"
    ]
  },
  {
    "objectID": "Independent t test.html#independent-t-test-example",
    "href": "Independent t test.html#independent-t-test-example",
    "title": "Independent t-test",
    "section": "Independent t-test example",
    "text": "Independent t-test example\nFor the t test example, we’ll use a dataset based on a fictional experiment involving Harry Potter and the cloak of invisibility. The cloak of invisibility if the one of the deathly hallows and a gift that Harry received in year one at Hogwarts. When a person puts on the cloak they are invisible to everyone and can run around the castle doing whatever kinds of mischief then can imagine.\n\nHypotheses\nSo the experiment involves the use of the cloak and if persons act more mischievous when they wear it. So let’s lay out the particulars of the experiment.\n\nIndependent variable (IV) = The Cloak\nDependent variable (DV) = Acts of mischief\n\nSince this is an independent t test there will be 2 groups.\n\nGroup 1 - Experimental Group - Wears the cloak of invisibility\nGroup 2 - Control Group - Does not wear a cloak of invisibility\n\nThis would call for two hypotheses.\n\nNull Hypothesis - Wearing the invisibility cloak does not increase mischievous acts\nAlternative Hypothesis - Wearing the invisibility cloak increases mischievous acts\n\nThe experimental assumptions would be:\n\nNull hypothesis -&gt; Group 1 = Group 2 or \\(\\bar X_{1}=\\bar X _{2}\\)\nAlternative Hypothesis -&gt; Group 1 \\(\\neq\\) Group 2 or \\(\\bar X_{1}\\neq\\bar X _{2}\\)\n\n\n\nLoad the Data into R Studio\n\nFirst Step, upload dataset from SPSS\nGet data set named “Invisibility” from SPSS datasets\nUse import dataset tool under the environment tab\nFind file called invisibility.sav\n\n\nlibrary(haven)\nInvisibility &lt;- read_sav(\"Invisibility.sav\")\nView(Invisibility)\n\n\n\nInspect variables\n\nInvisibility$Cloak\n\n&lt;labelled&lt;double&gt;[24]&gt;: Cloak of invisibility\n [1] 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n\nLabels:\n value    label\n     0 No Cloak\n     1    Cloak\n\nInvisibility$Mischief\n\n [1] 3 1 5 4 6 4 6 2 0 5 4 5 4 3 6 6 8 5 5 4 2 5 7 5\nattr(,\"label\")\n[1] \"Mischievous Acts\"\nattr(,\"format.spss\")\n[1] \"F8.0\"",
    "crumbs": [
      "Intro to Statistics",
      "Independent t-test"
    ]
  },
  {
    "objectID": "Independent t test.html#bar-graph",
    "href": "Independent t test.html#bar-graph",
    "title": "Independent t-test",
    "section": "Bar Graph",
    "text": "Bar Graph\nTo begin, let’s analyze some descriptive statistics based on the “Invisibility” dataset and create a bar graph to have a first estimate as to whether there is a difference between the groups.\nThe first step is to create a second dataset with your descriptive variables. We’ll use the dplyr package to do this, which is part of tidyverse.\n\nFind Descriptive Statistics\n\nlibrary(dplyr)\nInvis_Descriptives &lt;- Invisibility |&gt;\n  group_by(Cloak) |&gt;\n  summarize(n = n(),\n            mean = mean(Mischief),\n            sd = sd(Mischief),\n            se = sd / sqrt(n),\n            ci = qt(0.975, df = n - 1) * sd / sqrt(n))\nInvis_Descriptives\n\n# A tibble: 2 × 6\n  Cloak            n  mean    sd    se    ci\n  &lt;dbl+lbl&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0 [No Cloak]    12  3.75  1.91 0.552  1.22\n2 1 [Cloak]       12  5     1.65 0.477  1.05\n\n\nNotice the basic structure of the code. We use the “filter” or “pipe” function to filter the dataset based on the two groups and then find all the important descriptive statistics we need. When we take a closer look at this dataset a few things should be of interest.\n\nInvis_Descriptives\n\n# A tibble: 2 × 6\n  Cloak            n  mean    sd    se    ci\n  &lt;dbl+lbl&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0 [No Cloak]    12  3.75  1.91 0.552  1.22\n2 1 [Cloak]       12  5     1.65 0.477  1.05\n\n\nThe first thing we should notice is that the mean number of mischievous acts without the cloak is 3.75, while the mean number of mischievous acts with the cloak is 5.00. Remember the assumption of the alternative hypothesis.\n\nAlternative Hypothesis - Group 1 \\(\\neq\\) Group 2 or \\(\\bar X_{1}\\neq\\bar X _{2}\\)\n\nAnd this is what we find when we compare the two means\n\\[\n3.75\\neq 5.00\n\\]\nThis is also demonstrated with a basic bar graph as well.\n\nggplot(Invis_Descriptives, \n       aes(x = Cloak, \n           y = mean)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\n\n\nStatistically Significant?\nSo it does look like there is a difference between these two groups. The question remains, is the difference statistically significant. Thinking back to the t-test equation, is the mean difference significantly more than the measurement error to produce a t score that is associated with a low probability?\n\\[\nt = \\frac{5.00-3.75}{measurement\\; error}\n\\]\nRemember in this case, the measurement error statistic we are using is the estimated standard error, so the formula looks like this:\n\\[\nt = \\frac{5.00-3.75}{estimated\\; standard\\; error}\n\\]\nTo find the estimated standard error, the formula is:\n\\[\n\\sqrt{\\frac{s^{2}_{1}}{n_{1}}+\\frac{s^{2}_{2}}{n_{2}} }\n\\]\nIf we were going to run the t score calculation by hand it would look like this:\n\\[\nt = \\frac{5.00-3.75}{\\sqrt{\\frac{1.91^2}{12}+\\frac{1.65^2}{12 }}}\n\\]So we could do the t test by hand using r\n\n(3.75-5.00)/(sqrt(1.91^2/12 + 1.65^2/12))\n\n[1] -1.715578\n\n\nRather than needing to run the calculation by hand, we can use r to the the calculation for us using the t.test() function.\n\nt.test(Mischief ~ Cloak, data = Invisibility)\n\n\n    Welch Two Sample t-test\n\ndata:  Mischief by Cloak\nt = -1.7135, df = 21.541, p-value = 0.101\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -2.764798  0.264798\nsample estimates:\nmean in group 0 mean in group 1 \n           3.75            5.00 \n\n\nSo we can see that the t score is -1.71 and the corresponding p value is .1007. The p value is greater than alpha or greater than .05. Thus, we fail to reject the null and our experiment does not support the idea that a cloak of invisibility increases acts of mischief.\n\n\nT Distribution for the Example\nThe degrees of freedom (df) is 22, so the t distribution would take on a shape that looks like this:\n\n\n\n\n\n\n\n\n\nHere’s a graph that shows the area for rejection of the Null (p value &lt; .05) or retaining the null (p value &gt; .05).\n\ndist_t(deg.f = 22, p = 1)\n\n\n\n\n\n\n\n\nNotice that the t score that is the start of the rejection area for the Null is 1.72, which is just slightly bigger than 1.71 which is the t score associated with the outcome in the invisibility cloak experiment, which makes sense since the p value was .1007 only .05 away from the alpha level.\n\n\nEffect Size\nThe other statistic that will be a part of the conclusion is the effect size, which is a measurement of the magnitude of the results obtained in the experiment. Effect sizes can be used to compare the magnitude of this experiment to others and it has become standard practice to include an effect size with any statistical results.\nIn this case, we’ll use Cohen’s d as a measure of the effect size. The formula for Cohen’s d is to subtract the means from each other and then divide by the standard deviation of the control group.\nWe can bring up our descriptive statistics if we need to remember the means and standard deviations.\n\nInvis_Descriptives\n\n# A tibble: 2 × 6\n  Cloak            n  mean    sd    se    ci\n  &lt;dbl+lbl&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0 [No Cloak]    12  3.75  1.91 0.552  1.22\n2 1 [Cloak]       12  5     1.65 0.477  1.05\n\n\nSo for Cohen’s d\n\nCohens_d &lt;- (5.00-3.75)/1.91\nCohens_d\n\n[1] 0.6544503\n\n\n\n\nBar Graph for t Test\nEarlier, we looked at a basic bar graph that showed a difference between the means. However, a key piece of information was missing from this graph, confidence intervals. Let’s go ahead and add those in.\n\nggplot(Invis_Descriptives, \n       aes(x = Cloak, \n           y = mean)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin=mean-ci,\n                    ymax=mean+ci))\n\n\n\n\n\n\n\n\nRemember that confidence intervals show a range or interval for values of the population mean based on a sample. For an Independent t test, if the independent variable has a real effect, we would expect that the two samples come from different populations, rather than the same population. When the confidence intervals overlap as they do in the chart above that means that there is a substantial chance that these samples came from the same population. If the confidence intervals do not overlap there is a much smaller chance that they came from the same population, thus the independent variable is more likely to have had an effect.\nWe use confidence intervals, p values, and effect sizes to analyze an experiment. A p value may be below .05, but the confidence intervals may still be touching, so all parts of the analysis are helpful.\nWe can add in labels to improve the look of our chart. We’ve added labels to our factor variable and labels for the title and x and y variables.\n\nggplot(Invis_Descriptives, \n       aes(x = factor(Cloak, labels=c(\"No Cloak\", \"Cloak\")),\n           y = mean)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin=mean-ci,\n                    ymax=mean+ci)) +\n  labs(title = \"Mean Number of Mischievious Acts with or without Cloak\", \n       y=\"Mean Number of Mischievious Acts\", x=\"Were They Wearing a Cloak?\")\n\n\n\n\n\n\n\n\nNext let’s add some color to our chart\n\nggplot(Invis_Descriptives, \n       aes(x = factor(Cloak, labels=c(\"No Cloak\", \"Cloak\")),\n           y = mean)) +\n  theme_minimal() +\n  geom_bar(stat = \"identity\", fill=\"cornflowerblue\") +\n  geom_errorbar(aes(ymin=mean-ci,\n                    ymax=mean+ci), width=.3, size=1) +\n  labs(title = \"Mean Number of Mischievious Acts with or without Cloak\", \n       y=\"Mean Number of Mischievious Acts\", x=\"Were They Wearing a Cloak?\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nWriting up the Results\nHere is how to write up the results for this experiment including all the relevant information. Of course, it’s also important to include the bar graph as well.\n\n\nResults\n\nOn average, participants given a cloak of invisibility engaged in more acts of mischief (M = 5, SE = 0.48), than those not given a cloak (M = 3.75, SE = 0.55). This difference, 1.25, 95% CI[-2.76, 0.26], was not significant t(21.54) = −1.71, p = 0.101. However, it did represent a medium-sized effect d = 0.65.*\n\nSteps to writing results\n\nWrite out the means and include standard error\nWrite out the difference between the means (Subtract the sample means) and the confidence intervals for the difference, which is a part of the r output.\nt(df)= t score, p value, Cohen’s d",
    "crumbs": [
      "Intro to Statistics",
      "Independent t-test"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Assignment Template.html",
    "href": "Assignment Template.html",
    "title": "Homework #1 score (Assignment description)",
    "section": "",
    "text": "Create a new dataset based on the following information using the data.frame function. You’re interested in determining whether the birth control has the side effect of changing blood pressure. Here is the data.\n\n\n\n\nSubject\nBirth Control Pill\nPlacebo\n\n\n\n\n1\n108\n102\n\n\n2\n76\n68\n\n\n3\n69\n66\n\n\n4\n78\n71\n\n\n5\n74\n76\n\n\n6\n85\n80\n\n\n7\n79\n82\n\n\n8\n78\n79\n\n\n9\n80\n78\n\n\n10\n81\n85\n\n\n\n\nA nurse was hired by an agency to investigate the impact of a lead smelter on the level of lead in the blood of children living near the smelter. Here is the level of blood in children in the data she collected. Use the tibble function to create a data frame.\n\n\n\n\nLive near the Smelter\nLive in another area\n\n\n\n\n16\n9\n\n\n18\n13\n\n\n21\n8\n\n\n14\n15\n\n\n17\n17\n\n\n19\n12\n\n\n22\n11\n\n\n24\n16\n\n\n15\n12\n\n\n18\n15"
  },
  {
    "objectID": "Working with Quarto.html",
    "href": "Working with Quarto.html",
    "title": "Working with Quarto",
    "section": "",
    "text": "Quarto is a document format in R Studio that allows you to print off your notes, homework, and code as a word doc or other types of files. It is based on R markdown, which is a prior version of this software. You can also make websites, slide decks, and other formats to present your findings from your statistical research. For our class we’ll be using it for homework, quizzes, and tests when we’re working on problems that involve data analysis.\n\n\n\n\n\n\n\nFor example, the penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\n\n\n\nYou can use the Palmer Penguins dataset to make a graph or plot of the data.\nThe graph below shows the relationship between flipper and bill lengths of these penguins.\n\n\n\n\n\n\n\n\n\nNotice how Quarto runs the code to create your graph and then includes it in the output.\nHere is a tutorial for Quarto using R Studio\n\n\nWhen you want to preview your content you hit the render button and it will show you how you are doing so far.\n\n\n\nIf you go up to the insert feature in the tool bar, you can insert a cell like the one below where you can include R code. Or you can use the hot keys Option+Command+I for the Mac or in Windows Control+Alt+I\nThen you can put code into the cell to run it in the document\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nIf you click the green arrow on the right of the cell, it will run the code chunk for you and you can check it to make sure it’s right.\n\n\n\n\nYou can download the assignment template to use when you are working on your assignment. Here is how to set it up.\n\n\n\nThis is just the title of the assignment and your name, date, and the output, which will be a word document. This is always at the top of any Quarto document.\n\n\n\nThe r setup file lets you add any libraries or datasets you may need for the file. For example, you’ll need to usually add the tidyverse library for most of the assignments.\nYou’ll also need to add any datasets you are working with that you need to import. For example, to add SPSS files like the Album Sales Dataset you’ll need to add the haven library and then include the code to import the file.\nOr if you import a .csv file, you’ll need to include the code for that as well.\nIf you’re not sure of where the file is, you can always import the file first and the copy the code from your console. Typically this is not included (include = FALSE), but sometimes we need to see how you set things up.\n\n\n\nFor each question you’ll want to copy and paste the question from your homework into R Markdown.\n\n\n\nQuarto allows you to answer the question using both the code and output. So I’ll start with a descriptive sentence or annotation about what I’m doing and then show the code and the output.\nFilter dataset to mass below 250\n\nSmaller_StarWars &lt;- starwars |&gt;\n  filter(starwars$mass &lt; 250)\nSmaller_StarWars\n\n# A tibble: 58 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 48 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nCreate a scatterplot of mass and height\n\nggplot(data = Smaller_StarWars) +\n  geom_point(mapping = aes(x = mass, \n                           y = height))\n\n\n\n\n\n\n\n\n\n\n\nAfter your done with your document and want to turn it in, you’ll click the render button and it will open as a word document.\nThe document will be saved in your working directory. The working directory is the file where all your current files are being saved. It should be listed in your console or you can type the command getwd() to find it as well.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#palmer-penguins",
    "href": "Working with Quarto.html#palmer-penguins",
    "title": "Working with Quarto",
    "section": "",
    "text": "For example, the penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#graphs",
    "href": "Working with Quarto.html#graphs",
    "title": "Working with Quarto",
    "section": "",
    "text": "You can use the Palmer Penguins dataset to make a graph or plot of the data.\nThe graph below shows the relationship between flipper and bill lengths of these penguins.\n\n\n\n\n\n\n\n\n\nNotice how Quarto runs the code to create your graph and then includes it in the output.\nHere is a tutorial for Quarto using R Studio\n\n\nWhen you want to preview your content you hit the render button and it will show you how you are doing so far.\n\n\n\nIf you go up to the insert feature in the tool bar, you can insert a cell like the one below where you can include R code. Or you can use the hot keys Option+Command+I for the Mac or in Windows Control+Alt+I\nThen you can put code into the cell to run it in the document\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nIf you click the green arrow on the right of the cell, it will run the code chunk for you and you can check it to make sure it’s right.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#how-to-set-up-an-assignment",
    "href": "Working with Quarto.html#how-to-set-up-an-assignment",
    "title": "Working with Quarto",
    "section": "",
    "text": "You can download the assignment template to use when you are working on your assignment. Here is how to set it up.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#write-your-yaml-file",
    "href": "Working with Quarto.html#write-your-yaml-file",
    "title": "Working with Quarto",
    "section": "",
    "text": "This is just the title of the assignment and your name, date, and the output, which will be a word document. This is always at the top of any Quarto document.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#add-the-r-setup-file",
    "href": "Working with Quarto.html#add-the-r-setup-file",
    "title": "Working with Quarto",
    "section": "",
    "text": "The r setup file lets you add any libraries or datasets you may need for the file. For example, you’ll need to usually add the tidyverse library for most of the assignments.\nYou’ll also need to add any datasets you are working with that you need to import. For example, to add SPSS files like the Album Sales Dataset you’ll need to add the haven library and then include the code to import the file.\nOr if you import a .csv file, you’ll need to include the code for that as well.\nIf you’re not sure of where the file is, you can always import the file first and the copy the code from your console. Typically this is not included (include = FALSE), but sometimes we need to see how you set things up.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#add-questions-from-the-assignment.",
    "href": "Working with Quarto.html#add-questions-from-the-assignment.",
    "title": "Working with Quarto",
    "section": "",
    "text": "For each question you’ll want to copy and paste the question from your homework into R Markdown.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#answer-the-question",
    "href": "Working with Quarto.html#answer-the-question",
    "title": "Working with Quarto",
    "section": "",
    "text": "Quarto allows you to answer the question using both the code and output. So I’ll start with a descriptive sentence or annotation about what I’m doing and then show the code and the output.\nFilter dataset to mass below 250\n\nSmaller_StarWars &lt;- starwars |&gt;\n  filter(starwars$mass &lt; 250)\nSmaller_StarWars\n\n# A tibble: 58 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 48 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nCreate a scatterplot of mass and height\n\nggplot(data = Smaller_StarWars) +\n  geom_point(mapping = aes(x = mass, \n                           y = height))",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Working with Quarto.html#render-your-document",
    "href": "Working with Quarto.html#render-your-document",
    "title": "Working with Quarto",
    "section": "",
    "text": "After your done with your document and want to turn it in, you’ll click the render button and it will open as a word document.\nThe document will be saved in your working directory. The working directory is the file where all your current files are being saved. It should be listed in your console or you can type the command getwd() to find it as well.",
    "crumbs": [
      "R Basics",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Assignment Template (1).html",
    "href": "Assignment Template (1).html",
    "title": "Setup",
    "section": "",
    "text": "Setup\n\nAdd packages\nAdd External Datasets\n\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nAdd Questions from homework\n\nCreate a data set of the total number of Covid cases in 5 counties in California.\n\n\nTotal_Cases &lt;- tribble(~County, ~Cases, \n                       \"Los Angeles\", 2730000, \n                       \"Fresno\", 233000, \n                       \"Stanislaus\", 129000,\n                       \"Tulare\", 124000, \n                       \"San Diego\", 750000)\nTotal_Cases\n\n# A tibble: 5 × 2\n  County        Cases\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Los Angeles 2730000\n2 Fresno       233000\n3 Stanislaus   129000\n4 Tulare       124000\n5 San Diego    750000\n\n\n(Write a summary statement or add any other details if necessary)\n\nCreate a bar graph based on the Covid cases by county dataset you just created.\n\n\nggplot(data = Total_Cases) + \n  geom_bar(mapping = aes(x = County, y = Cases), \n           stat = \"identity\") +\n  labs(title = \"Total Cases by County in California\")"
  },
  {
    "objectID": "Descriptive Statistics.html",
    "href": "Descriptive Statistics.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Sometimes pre-existing datasets are imported into R Studio. There are several different types of datasets that can be imported. The ones used most frequently in this course are from Excel and SPSS. Types of computer files are indicated by what follows the period (“.”) in the file name. For example file.docx is a type of word file.\nHere’s a list of common file types we’ll use.\n\nExcel = .xlsx\nSPSS = .sav\nComma Seperated Values = .csv\n\nOne files we’ll be using today is the Album Sales file. On the moodle page for the course at the very top underneath the link for zoom sessions, you’ll see a folder with SPSS Data Sets. When you click on the folder you’ll see a list of datasets. Click on the one that says, “Album Sales.sav” and download it to your computer.\nOnce the file is downloaded go to the upper right pane in R Studio. Under the environment tab you’ll see a button that says “Import Dataset”. Click on the button and you’ll get several different options. Go to the one that says “From SPSS” and click on it. At the top of the window you’ll need to find the file you are importing to open it. Click on the Browse buttom to find the file. Once you find it click open and it will give you a preview of the file in the window. Then all you need to do is click import.\nHere is the code for importing the file\n\nlibrary(haven)\nAlbum_Sales &lt;- read_sav(\"Album Sales.sav\")\nView(Album_Sales)",
    "crumbs": [
      "Intro to Statistics",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "Descriptive Statistics.html#opening-datasets",
    "href": "Descriptive Statistics.html#opening-datasets",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Sometimes pre-existing datasets are imported into R Studio. There are several different types of datasets that can be imported. The ones used most frequently in this course are from Excel and SPSS. Types of computer files are indicated by what follows the period (“.”) in the file name. For example file.docx is a type of word file.\nHere’s a list of common file types we’ll use.\n\nExcel = .xlsx\nSPSS = .sav\nComma Seperated Values = .csv\n\nOne files we’ll be using today is the Album Sales file. On the moodle page for the course at the very top underneath the link for zoom sessions, you’ll see a folder with SPSS Data Sets. When you click on the folder you’ll see a list of datasets. Click on the one that says, “Album Sales.sav” and download it to your computer.\nOnce the file is downloaded go to the upper right pane in R Studio. Under the environment tab you’ll see a button that says “Import Dataset”. Click on the button and you’ll get several different options. Go to the one that says “From SPSS” and click on it. At the top of the window you’ll need to find the file you are importing to open it. Click on the Browse buttom to find the file. Once you find it click open and it will give you a preview of the file in the window. Then all you need to do is click import.\nHere is the code for importing the file\n\nlibrary(haven)\nAlbum_Sales &lt;- read_sav(\"Album Sales.sav\")\nView(Album_Sales)",
    "crumbs": [
      "Intro to Statistics",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "Descriptive Statistics.html#descriptive-statistics",
    "href": "Descriptive Statistics.html#descriptive-statistics",
    "title": "Descriptive Statistics",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nDescriptive statisitics are just what the name implies, they are used to describe a dataset. This is different from inferential statistics, which are used to infer population paramenters based on a sample.\nOne of the most common sets of descriptive statistics is known as central tendency, which is simply finding a way to get a basic overview of a dataset as a whole.\n\nMean\nThe easiest way to measure central tendency is to find the average or the mean. The formula for the mean is very straightforward. It’s simply the sum of all the scores divided by the number of scores. The symbol for the sample mean is an x with a bar over it. \\[\n\\bar X=\\frac{\\Sigma X}{N}\n\\] The capital Greek letter Sigma in the numerator stands for sum and X stands in for all the values in a dataset with the N standing for the number of values in the dataset. In R Studio, we simply use the mean command to find the mean for a particular variable\n\nmean(diamonds$carat)\n\n[1] 0.7979397\n\n\n\n\nMedian and Mode\nThere are two other measures of central tendency that are often used in statistics, median and mode. The median is simply the center score, so to find it, you simply rank order your values from lowest to highest and the middle score in that rank order. For even scores you take the two middle scores add them and divide them by two. For odd scores there is only one middle score. Here again, R gives us an easy way to find the median.\n\nmedian(diamonds$carat)\n\n[1] 0.7\n\n\nThe mode is the score or value with the highest frequency in your dataset. So whatever value occurs the most times is the mode. For some reason, mode is not part of the basic R package, so you need to install the “modeest” package if you need to find this descriptive statistic.\n\nlibrary(modeest)\n\nOnce the library is installed use the mfv command\n\nmfv(diamonds$carat)\n\n[1] 0.3\n\n\nWe tend to use the mean most often for central tendency, but it’s more effected by extreme scores. So if you have several low or high outliers (extreme scores that are higher or lower than the mean), the median may be a more accurate representation of the data. More often then not, we’ll use the mean. The mode is a more straightforward measure. So if you were interested in the most popular song on an album based on downloads, you would find the mode. The mode is often very helpful with categorical data.",
    "crumbs": [
      "Intro to Statistics",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "Descriptive Statistics.html#variability",
    "href": "Descriptive Statistics.html#variability",
    "title": "Descriptive Statistics",
    "section": "Variability",
    "text": "Variability\nThe second most important aspect of descriptive statistics is variability or dispersion in a dataset. This typically represents how the rest of the dataset relates to the mean or some other measure of central tendency. For example: are the scores close to or widely dispersed from the mean?\n\nStandard Deviation\nOften times were interested in the average spread or deviation from the mean. Deviance is the distance from any particular score from the mean. So to find the deviance you simply take the score and subtract it from the mean. \\[\ndeviance = X - \\bar X\n\\] If we wanted to find out the total amount of deviance, we could simply add together the total deviance for each number in our dataset. So the equation would look like this: \\[\ntotal\\;deviance = \\Sigma(X-\\bar X)\n\\] Unfortunately this equation causes some problems. If you remember, the mean is the average score, so it’s the score at around 50%. But that means that for any dataset about half the scores will be above the mean and half the scores will be below the mean. Or another way to think about it, half the deviation scores will be negative and half the deviation scores will be positive. So if we add up all these scores, the total deviation will be equal to 0, but 0 doesn’t tell us much about the spread of the scores.\nThe way to overcome this problem is by squaring each deviation score, which makes all the deviation values positive and thus produces a positive number. This number is called the sum of squared errors of the sum of squares (SS) with the following formula. \\[\nsum\\;of\\;squares (SS) = \\Sigma(X-\\bar X)^2\n\\] This number is still somewhat inflated, since its constructed based on squared values. One way to fix this issue to to find the average dispersion, which will be based on the number in our sample. Since the sample is an estimate of the population, we actually don’t use N, but N -1. This number is called variance and has the following symbol and equation. \\[\nvariance(s^2) = \\frac {SS}{N-1} = \\frac {\\Sigma(X-\\bar X)^2}{N-1}\n\\]\nThis number is closer to the original units of measurement, but to make it more accurate, the original squared values need to be taken back out of the measure. To do this, the square root of the variance is calculated.\n\\[\ns = \\sqrt {\\frac {\\Sigma (X- \\bar X)^2}{N-1}}\n\\]\nTo find the standard deviation using R Studio we use the sd command to find it.\n\nsd(Album_Sales$Adverts)\n\n[1] 485.6552\n\n\n\n\nRange\nAnother measure of dispersion is range. This is simply subtracting the highest score from the lowest score for a particular variable or vector of scores. So to find the range simply use the range function and then subtract the two numbers provided, which are the highest and the lowest.\n\nrange(Album_Sales$Adverts)\n\n[1]    9.104 2271.860\n\n\nThen simply subtract the scores\n\n2271.860-9.104\n\n[1] 2262.756\n\n\n\n\nInterquartile Range\nAnother helpful type of range is the interquartile range, which is the range of numbers from the 25th and 75th percentile. Percentiles are just dividing up a dataset based on where certain scores are based on percentages. For example, we can look at what score is at the 50th percentile. To do that we use the quantile function (a quantile is the same thing as a percentile). x is the variable we are analyzing, and .5 is the percentile (.5 = 50%; .35 = 35%)\n\nquantile(x = diamonds$carat, probs = .5)\n\n50% \n0.7 \n\n\nSo the number at the 50th percentile is 0.7. Notice that this is the same as the median, described earlier. Remember that the median is the number from the dataset that is in the middle or center of the dataset. Because the mean is calculated, it may or may not be a number in the dataset. The mean is the number that describes the closest number to the average, but may or may not be a number contained in the dataset.\nSo to find the interquartile range, we want our two quartiles or numbers at “quarters” of the dataset, so 25% and 75%. Think of a dollar, which is made up of 4 quarters. We already know that half a dolar is 50 cents, which would be the median, but the other two quartiles (quarters) would be 25 cents and 75 cents.\n\nquantile(x = diamonds$carat, probs = c(.25, .75))\n\n 25%  75% \n0.40 1.04 \n\n\nFinally, to get the interquartile ranks we simply subtract these two numbers.\n\n1.04-.40\n\n[1] 0.64\n\n\nSo the interquartile range is 0.64.",
    "crumbs": [
      "Intro to Statistics",
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "Correlation.html",
    "href": "Correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "One of the first tests we can use to look at the relationship between two variables is correlation. Correlation usually requires two continuous numerical variables of some kind.\n\nVariance\nThe basic method for comparison between the two variables is variance, which we learned when looking at descriptive statisitics. Remember that variance is a measurement of data dispersion or spread. More specifically it refers to the average amount that data varies from the mean. So what correlation ultimately analyses is whether 2 variables vary in a similar way from their respective means. Remember here’s the formula for variance.\n\\[\nvariance(s^2)=\\frac{\\Sigma(x-\\bar x)^2}{N-1}\n\\]\n\n\nCovariance\nSince correlation is based on how two variables vary, naturally we are interested in covariance. Covariance is simply variance for two variables. Notice here in the formula that we’ve modified the orinigal variance formula and included our second variable y. So we’ve modified the exponent in the formula for x - the mean and incorporated y. \\[\ncovariance(x,y)=\\frac{\\Sigma(x-\\bar x)(y-\\bar y)}{N-1}\n\\]\nx stands for our first variable and y stands for our second variable, so we are analyzing variance over two variables rather than just one. How does one variable deviate from the mean in comparison to how another variable deviates from the mean? If they deviate (vary) from the mean in a similar way they will be expected to be highly correlated. When there are two variables we can multiply the deviation for one variable by the devation from the second variable. If both deviations are either positive or negative this will give us a positive value, which tells us that the deviations are in the same direction (positive correlation). If the deviations go in opposite directions (one positive and one negative) this will give us a negative value (negative corrleation). Multiplying deviations of one variable by a second variable gets us the cross-product deviation.\nAt this point the covariance is dependent upon the types of units used to calculate the number. However, we want to standardize this number, which basically means that the number will be in units that are the same across different experiments and the tools they use to measure their variables.In this case to standardize our covariance we use a calculation discussed in descriptive statistics, the standard deviation.\n\n\nPearson’s r\nThis gets us what’s known as Pearson’s r, which was named after the person who developed the calculation, Karl Pearson with Florenece Nightingale David doing a lot of the harder mathematical calculations. So hear is the formula for Pearson’s r \\[\nr = \\frac {cov_{xy}}{s_{x}s_{y}}=\\frac{\\Sigma(x-\\bar x)(y-\\bar y)}{(N-1)s_{x}s_{y}}\n\\]\nNotice that the formula is the covariance of x and y divided by the standard deviation of x and y, then this is elaborated in the longer version following the second equal sign where the standard deviation for x and y is added to our original formula for covariance.\n\n\nCorrelational Coefficent\nPearon’s r or the correlational coefficient always varies between +1 and -1. As your r value moves closer to +1 that means that both variables are varying in the same direction either both increasing or both descreasing. When your r value is moving closer to -1 that indicates that both scores are varying in opposite directions or as one varible increases the other decreases.\nCorrelation only describes relationships between variables, not causal relationships between variables. It’s not possible to decipher which variable is doing the causal work or which variable is the independent variable and which is the dependent variable. However, correlation can still provide helpful information about variables and we can still use them to test certain types of hypotheses.\n\n\nExploring Correlation in a Dataset\nTo explore correlation futher, let’s look at a particular dataset. Open the dataset labeled album_sales.csv. Use the Import Dataset command in the upper righthand window and let’s look at the data set.\n\nhead(Album_Sales)\n\n# A tibble: 6 × 4\n  Adverts Sales Airplay Image\n    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1    10.3   330      43    10\n2   986.    120      28     7\n3  1446.    360      35     7\n4  1188.    270      33     7\n5   575.    220      44     5\n6   569.    170      19     5\n\n\n\n\nAlbum Sales Dataset\nThis is a dataset for a record company looking at various variables related to album sales.\n\nAdverts - Money spent on advertising\nSales - Sales for various albums\nAirplay - how much airplay each album received\nAttract - How attractive each band was on a 1-10 scale\n\nNext let’s look at the types of variables we have\n\nstr(Album_Sales)\n\ntibble [200 × 4] (S3: tbl_df/tbl/data.frame)\n $ Adverts: num [1:200] 10.3 985.7 1445.6 1188.2 574.5 ...\n  ..- attr(*, \"label\")= chr \"Advertsing budget (thousands)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n $ Sales  : num [1:200] 330 120 360 270 220 170 70 210 200 300 ...\n  ..- attr(*, \"label\")= chr \"Album sales (thousands)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.0\"\n $ Airplay: num [1:200] 43 28 35 33 44 19 20 22 21 40 ...\n  ..- attr(*, \"label\")= chr \"No. of plays on radio\"\n  ..- attr(*, \"format.spss\")= chr \"F8.0\"\n $ Image  : num [1:200] 10 7 7 7 5 5 1 9 7 7 ...\n  ..- attr(*, \"label\")= chr \"Band image rating (0-10)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.0\"\n\n\nNotice that each of the variables are continuous, either numeric (num) or integer (int), so this is a great dataset for doing correlations.\nHow would you expect attractiveness of the band to be related to album sales? Unless you’ve been living under a rock or don’t know anything about rock `n roll, attractiveness is a huge part of album sales. Whether your Dua Lipa or BTS, looks matter if your in the entertainment industry. So we should expect that as an artists looks increase, their album sales should increase as well.\n\n\nScatterplot - Band Attractiveness & Album Sales\nLet’s use a graph to first look at this data using a ggplot we learned earlier called a scatterplot. We’ll do a basic one first.\n\nggplot(Album_Sales, mapping = aes\n       (x = Image, y = Sales)) + geom_point()\n\n\n\n\n\n\n\n\nSo notice that dots lower on the attractiveness scale tend to be associated with less sales, whereas when you look at the dots higher on the attractiveness scales their are more dots that are higher on the sales scale.\n\n\nRegression Line\nAnother tool to help us analyze this relationship is a regression line. This is a line drawn on the graph that is closest to as many dots or points as possible. You add it like this.\n\nggplot(Album_Sales, mapping = aes\n       (x = Image, y = Sales)) + geom_point() +\n        geom_smooth(method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nA regression line is simply using a line to describe the relationship between the two variables. As the line increases in slope or steepness, that indicates a stronger relationship. If the line moves from the lower left to the upper right that indicates a positive correlation whereas if the line moves from the upper left hand corner to the bottom right that indicates a negative correlation. The more the line “flattens” or decreases in slope the less correlation there is between the two variables.\nLet’s look at a second variable using the graph to judge which one is more correlated. Use advertising rather than attractiveness this time.\n\nggplot(Album_Sales, mapping = aes(x = Adverts, \n        y =   Sales)) + geom_point()\n\n\n\n\n\n\n\n\nNotice how most of the dots are gathered in the middle of the graph and moving from lower left to upper right. Notice also that no points are in the bottom right hand corner. So lower sales does not appear to be related to higher amount of advertising. Let’s see the location of the regression line.\n\nggplot(Album_Sales, mapping = aes(x = Adverts, \n      y = Sales)) + geom_point() +\n      geom_smooth(method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNotice that the regression line seems slightly steeper on the adverts graph vs. the Image graph. But we can’t be totally sure. So let’s go ahead and use our Pearson’s r correlation coefficient to help us. The formula is really simple, it is just “cor” followed by the two variables you are analyzing.\n\ncor(Album_Sales$Sales, Album_Sales$Image)\n\n[1] 0.3261111\n\n\nOk, so the correlation between Sales and Attractiveness is .33, now let’s look at Sales and Advertising.\n\ncor(Album_Sales$Sales, Album_Sales$Adverts)\n\n[1] 0.5784877\n\n\nNotice how the corrleation has increased (meaning moved closer to + 1) from .33 to .58. This would seem to justify our hunch that their was a stronger correlation between Sales and Advertising than between Sales and Image (Looks aren’t everything apparently), based on the regression line for each graph.\n\n\nHow to use correlations to test hypotheses\nTo more formally use correlation to test a hypothesis, let’s look at another variable in the Album Sales dataset, airplay. This is how often a song gets played on the radio (FYI, the radio is an ancient device that old people used to listen to random music played using mysterious spooky radio waves). So your hypothesis would look something like:\n\nThere is a correlation between the airplay an album receives and the sales for the album.\n\nThe null hypothesis would simply negate the primary hypothesis:\n\nThere is no correlation between the airplay an album receives and the sales for the album.\n\nTo test the hypothesis we use the “cor.test” command and then specify each of the variables we are analyzing, so it looks like this:\n\ncor.test(Album_Sales$Sales, Album_Sales$Airplay)\n\n\n    Pearson's product-moment correlation\n\ndata:  Album_Sales$Sales and Album_Sales$Airplay\nt = 10.524, df = 198, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5018814 0.6810668\nsample estimates:\n      cor \n0.5989188 \n\n\nThen write the results using the output including the correlation coefficient, 95% confidence internal, and the p value.\nResults There was a significant correlation between the airply an album received and the sales for the album, r = .60, 95% CI [.50, .68], p &lt; .001.\nLet’s unpack these results a bit. The correlation is pretty high at .60. Remember that the closer a correlational coefficient is to either +/- 1 the stronger the correlation. We’ve also listed the confidence interval, which means that we are 95% confident that the correlation coefficient is between .50 and .68. The big problem that may show up here is if the CI range includes 0, which would mean that it’s possible that the correlation is zero, thus nullifying our primary hypothesis. The closer this range is the more accurate our correlation estimate and the closer to 1, the stronger the correlation. Finally, our p value is very low, so this shows us that the finding is significant.\nFinally, we want to include a scatterplot graph with the regression line to show the relationship we found.\n\nggplot(Album_Sales, mapping = aes(x = Airplay, y = Sales)) +\n         geom_point() +\n        geom_smooth(method = 'lm')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHere’s a second version with a few more bells and whistles. I made a different color for the scatter dots by inserting a color command and changed the size to make them a bit bigger. I did something similar with the line to make it thicker by adjusting the size and removed the CIs with the “se = FALSE” command. I also added labels through the “labs” command, make sure to use quotations around your titles and x and y descriptions.\n\nggplot(Album_Sales, mapping = aes(x = Airplay, y = Sales)) +\n        geom_point(size = 1.5, color = \"cornflower blue\") +\n        geom_smooth(method = 'lm', se=FALSE, linewidth = 1.5) + \n        labs(title = \"Scatterplot of the Relationship between Airplay\n             and Sales\", y = \"Album Sales (thousands)\", \n             x = \"Number of Plays on the Radio\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Statistical Tests",
      "Correlation"
    ]
  },
  {
    "objectID": "Working with R Markdown.html",
    "href": "Working with R Markdown.html",
    "title": "Working with R Markdown",
    "section": "",
    "text": "Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com or a nice slide presentation of what R Markdown can do is here.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nHere is the code to create a code chunk. Three left quotes or accents followed by braces with an r and then closing 3 accents.\nIf you click the green arrow on the right, it will run the code chunk for you and you can check it to make sure it’s right.\n\n\n\nCode Chunk\n\n\nSo the output will look like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "Working with R Markdown.html#r-markdown",
    "href": "Working with R Markdown.html#r-markdown",
    "title": "Working with R Markdown",
    "section": "",
    "text": "Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com or a nice slide presentation of what R Markdown can do is here.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nHere is the code to create a code chunk. Three left quotes or accents followed by braces with an r and then closing 3 accents.\nIf you click the green arrow on the right, it will run the code chunk for you and you can check it to make sure it’s right.\n\n\n\nCode Chunk\n\n\nSo the output will look like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "Working with R Markdown.html#including-plots",
    "href": "Working with R Markdown.html#including-plots",
    "title": "Working with R Markdown",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. Most of the time we’ll have this set to echo=TRUE so we can see your code."
  },
  {
    "objectID": "Working with R Markdown.html#how-to-set-up-an-assignment",
    "href": "Working with R Markdown.html#how-to-set-up-an-assignment",
    "title": "Working with R Markdown",
    "section": "How to set up an assignment",
    "text": "How to set up an assignment"
  },
  {
    "objectID": "Working with R Markdown.html#write-your-yaml-file",
    "href": "Working with R Markdown.html#write-your-yaml-file",
    "title": "Working with R Markdown",
    "section": "1. Write your YAML file",
    "text": "1. Write your YAML file\nThis is just the title of the assignment and your name, date, and the output, which will be a word document. This is always at the top of any R Markdown document.\n\n\n\nYAML front matter"
  },
  {
    "objectID": "Working with R Markdown.html#add-the-r-setup-file",
    "href": "Working with R Markdown.html#add-the-r-setup-file",
    "title": "Working with R Markdown",
    "section": "2. Add the r setup file",
    "text": "2. Add the r setup file\nThe r setup file lets you add any libraries or datasets you may need for the file. For example, you’ll need to usually add the tidyverse library for most of the assignments.\nYou’ll also need to add any datasets you are working with that you need to import. For example, to add SPSS files like the “Album Sales” Dataset you’ll need to add the “haven library” and then include the code to import the file.\nOr if you import a .csv file, you’ll need to include the code for that as well.\nIf you’re not sure of where the file is, you can always import the file first and the copy the code from your console. Typically this is not included (include = FALSE), but sometimes we need to see how you set things up.\n\n\n\nr setup file"
  },
  {
    "objectID": "Working with R Markdown.html#add-questions-from-the-assignment.",
    "href": "Working with R Markdown.html#add-questions-from-the-assignment.",
    "title": "Working with R Markdown",
    "section": "3. Add questions from the assignment.",
    "text": "3. Add questions from the assignment.\nFor each question you’ll want to copy and paste the question from your homework into R Markdown. For example, from your quiz.\n\nUse the “Star Wars” data from the tidy verse, use the “pipe” feature to filter the dataset to mass below 250. With the new dataset, create a scatterplot of mass and height."
  },
  {
    "objectID": "Working with R Markdown.html#answer-the-question",
    "href": "Working with R Markdown.html#answer-the-question",
    "title": "Working with R Markdown",
    "section": "4. Answer the question",
    "text": "4. Answer the question\nR Markdown allows you to answer the question using both the code and output. So I’ll start with a descriptive sentence or annotation about what I’m doing and then show the code and the output.\nFilter dataset to mass below 250\n\nSmaller_StarWars &lt;- starwars %&gt;%\n  filter(starwars$mass &lt; 250)\nSmaller_StarWars\n\n# A tibble: 58 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 48 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nCreate a scatterplot of mass and height\n\nggplot(data = Smaller_StarWars) +\n  geom_point(mapping = aes(x = mass, \n                           y = height))"
  },
  {
    "objectID": "Working with R Markdown.html#knit-your-document",
    "href": "Working with R Markdown.html#knit-your-document",
    "title": "Working with R Markdown",
    "section": "5 Knit your document",
    "text": "5 Knit your document\nAfter your done with your document and want to turn it in, you’ll click the knit button and knit it as a word document.\nThe document will be saved in your working directory. The working directory is the file where all your current files are being saved. It should be listed in your console or you can type the command getwd() to find it as well.\nHere is a R Markdown template file to get you started:\nAssignment Template"
  },
  {
    "objectID": "Building Databases.html",
    "href": "Building Databases.html",
    "title": "Building Databases",
    "section": "",
    "text": "Scientific Notation\nWhen you first get R and R Studio set up, it may be using scientific notation to express larger numbers. So you’ll see numbers like this \\[\n5.234e+10\n\\] This is a type of exponent, which is in a scientific notation format. Here’s a simpler example to understand what this means. Let’s start with a number like 28.\nIn scientific notation this would look like \\[\n2.8e+01\n\\] Or in an exponential form more familiar \\[\n2.8x10^1\n\\] So it’s 2.8 times 10 to the first power. 280 Would look like this \\[\n2.8e+02/\nor/\n2.8x10^2\n\\] It’s basically an easier way to represent larger numbers like 280 million (280,000,000) \\[\n2.8e+08/\nor/\n2.8x10^8\n\\] To turn this off this setting do the following\n\noptions(scipen = 999)\n\nIf you want to turn it back on, do this\n\noptions(scipen = 0)\n\n\n\nMore work on databases\nLet’s make up a database based on covid figures from the New York Times.\n\nCreate your objects\nMake sure to use quotations for objects that are names or titles (Remember these are categorical variables)\n\n\nCountries &lt;- c(\"United States\", \"India\", \"Brazil\", \"Russia\", \"UK\")\n\n\nTotal_Cases  &lt;- c(24249722, 10581837,8511770,3574330, 3466849)\n\n\nTotal_Deaths &lt;- c(400810, 152556, 210299,65632,91470)\n\nThen you can use the data.frame command to put them all together\n\nCovid &lt;- data.frame(Countries, Total_Cases, Total_Deaths)\n\nYou could actually do all these steps at the same time\n\nCovid_Again &lt;- data.frame(Countries = c(\"United States\", \"India\", \n                          \"Brazil\", \"Russia\", \"UK\"), \n                          Total_Cases  = c(24249722, 10581837,8511770,\n                                           3574330, 3466849),  \n                          Total_Deaths = c(400810, 152556, 210299,65632,91470))\n\nAnother nice way to make a dataset is by using a tibble\nThis is part of the tidyverse package and simplifies the code somewhat. Notice that the command to make a tibble is actually tribble.\n\nCovid_TR &lt;- tribble(\n            ~Countries, ~Total_Cases, ~Total_Deaths, \n            \"United States\", 24249722, 400810, \n            \"India\", 10581837, 152556, \n            \"Brazil\", 8511770, 210299, \n            \"Russia\", 3574330, 65632, \n            \"UK\", 3466849, 91470\n)\n\nA tibble is nice because it sets it up more like a spreadsheet.\nNotice that the ~ specifies the columns or variables and then the rest are like rows.\n\n\nManipulate Data\nMortality rate is total deaths divided by the total number of cases. You can use R to calculate this for you and then create the object.\n\nMortality_Rate &lt;-  c(Total_Deaths/Total_Cases)\n\nThen we can add all four variables together to remake our covid data.frame\n\nCovid &lt;- data.frame(Countries,Total_Cases,Total_Deaths,Mortality_Rate)\n\nTidyverse supplies some other helps here if we are using tibbles.\nWe can use mutate to add in the other variable based on a computation.\n\nCovid_TR &lt;- mutate(Covid_TR, Mortality_Rate = Total_Deaths/Total_Cases)\n\nWe can use rename to change the name of our variable\n\nCovid_TR &lt;- rename(Covid_TR, Mortality = Mortality_Rate)\n\nYou can practice this on your own.",
    "crumbs": [
      "R Basics",
      "Building Databases"
    ]
  },
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "hello.html#meet-quarto",
    "href": "hello.html#meet-quarto",
    "title": "Hello, Quarto",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "hello.html#meet-the-penguins",
    "href": "hello.html#meet-the-penguins",
    "title": "Hello, Quarto",
    "section": "Meet the penguins",
    "text": "Meet the penguins\n\nThe penguins data from the palmerpenguins package contains size measurements for 344 penguins from three species observed on three islands in the Palmer Archipelago, Antarctica.\nThe plot below shows the relationship between flipper and bill lengths of these penguins."
  }
]